[{"html":"<h2 id=\"abstract\">Abstract</h2>\n<p>Digital Marketing Systems (DMS) are the primary point of contact between a digital business and its customers. In this context, the communication channel optimization problem poses a precious and still open challenge for DMS. Due to its interactive nature, Reinforcement Learning (RL) appears as a promising formulation for this problem. However, the standard RL setting learns from interacting with the environment, which is costly and dangerous for production systems. Furthermore, it also fails to learn from historical interactions due to the distributional shift between the collection and learning policies. For this matter, we present PulseRL, an offline RL-based production system for communication channel optimization built upon the Conservative Q-Learning (CQL) Framework. PulseRL architecture comprises the whole engineering pipeline (data processing, training, deployment, and monitoring), scaling to handle millions of users. Using CQL, PulseRL learns from historical logs, and its learning objective reduces the shift problem by mitigating the overestimation bias from out-of-distribution actions. We conducted experiments in a real-world DMS. Results show that PulseRL surpasses RL baselines with a significant margin in the online evaluation. They also validate the theoretical properties of CQL in a complex scenario with high sampling error and non-linear function approximation.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"/img/pulserl-architecture.png\">\n</div>\n\n<blockquote>\n<p>Illustration of PulseRL’s system pipeline. We compose it with different big data storage models, a data transformation engine, a task manager, and specialized microservices for training and inference, which ensures scalability for handling millions of users on a daily basis. It also provides version control for source code, dataset, MDP’s and RL agents.</p>\n</blockquote>\n<p><br/><br/></p>\n<p>More info soon.</p>\n","readingTime":"2 min read","title":"PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning","slug":"2021-10-25-pulse-rl","date":"2021-10-25","urls":[{"cta":"Code","url":"https://github.com/dlb-rl/pulse-rl"},{"cta":"Paper","url":"https://offline-rl-neurips.github.io/2021/pdf/9.pdf"},{"cta":"Dataset","url":"https://bit.ly/ac-dataset"}],"type":"Workshop Publication","tags":["research","publication"],"image":"/img/pulserl.png","description":"Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021)."},{"html":"<p>A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"/img/soccer.gif\">\n</div>\n\n<p>Pre-compiled versions of this environment are available for Linux, Windows and MacOS (x86, 64 bits). The source code for this environment is available <a href=\"https://github.com/bryanoliveira/unity-soccer\">here</a>.</p>\n<h2 id=\"requirements\">Requirements</h2>\n<p>See <a href=\"https://github.com/bryanoliveira/soccer-twos-env/blob/master/requirements.txt\">requirements.txt</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"for-training\">For training</h3>\n<p>Import this package and instantiate the environment:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n\nenv = soccer_twos.make()\n</code></pre>\n<p>The <code>make</code> method accepts several options:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>render</code></td>\n<td>Whether to render the environment. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>watch</code></td>\n<td>Whether to run an audience-friendly version the provided Soccer-Twos environment. Forces <code>render</code> to <code>True</code>, <code>time_scale</code> to <code>1</code> and <code>quality_level</code> to <code>5</code>. Has no effect when <code>env_path</code> is set. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>variation</code></td>\n<td>A soccer env variation in EnvType. Defaults to <code>EnvType.multiagent_player</code></td>\n</tr>\n<tr>\n<td><code>time_scale</code></td>\n<td>The time scale to use for the environment. This should be less than <code>100</code>x for better simulation accuracy. Defaults to <code>20</code>x realtime.</td>\n</tr>\n<tr>\n<td><code>quality_level</code></td>\n<td>The quality level to use when rendering the environment. Ranges between <code>0</code> (lowest) and <code>5</code> (highest). Defaults to <code>0</code>.</td>\n</tr>\n<tr>\n<td><code>base_port</code></td>\n<td>The base port to use to communicate with the environment. Defaults to <code>50039</code>.</td>\n</tr>\n<tr>\n<td><code>worker_id</code></td>\n<td>Used as base port shift to avoid communication conflicts. Defaults to <code>0</code>.</td>\n</tr>\n<tr>\n<td><code>env_path</code></td>\n<td>The path to the environment executable. Overrides <code>watch</code>. Defaults to the provided Soccer-Twos environment.</td>\n</tr>\n<tr>\n<td><code>flatten_branched</code></td>\n<td>If <code>True</code>, turn branched discrete action spaces into a <code>Discrete</code> space rather than <code>MultiDiscrete</code>. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>opponent_policy</code></td>\n<td>The policy to use for the opponent when <code>variation==team_vs_policy</code>. Defaults to a random agent.</td>\n</tr>\n<tr>\n<td><code>single_player</code></td>\n<td>Whether to let the agent control a single player, while the other stays still. Only works when <code>variation==team_vs_policy</code>. Defaults to <code>False</code>.</td>\n</tr>\n</tbody></table>\n<p>The created <code>env</code> exposes a basic <a href=\"https://gym.openai.com/\">Gym</a> interface.\nNamely, the methods <code>reset()</code>, <code>step(action: Dict[int, np.ndarray])</code> and <code>close()</code> are available.\nThe <code>render()</code> method has currently no effect and <code>soccer_twos.make(render=True)</code> should be used instead.\nThe <code>step()</code> method returns extra information about the player and the ball in the last tuple element.\nThis information may be used to build custom reward functions if needed.</p>\n<p>We expose an RLLib-compatible multiagent interface.\nThis means, for example, that <code>action</code> should be a <code>dict</code> where keys are integers in <code>{0, 1, 2, 3}</code> corresponding to each agent.\nAdditionally, values should be single actions shaped like <code>env.action_space.shape</code>.\nObservations and rewards follow the same structure. Dones are only set for the key <code>__all__</code>, which means &quot;all agents&quot;.\nAgents 0 and 1 correspond to the blue team and agents 2 and 3 correspond to the orange team.</p>\n<p>Here&#39;s a full example:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n\nenv = soccer_twos.make(render=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Observation Space: &quot;</span>, env.observation_space.shape)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Action Space: &quot;</span>, env.action_space.shape)\n\nteam0_reward = <span class=\"hljs-number\">0</span>\nteam1_reward = <span class=\"hljs-number\">0</span>\n<span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:\n    obs, reward, done, info = env.step(\n        {\n            <span class=\"hljs-number\">0</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">1</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">2</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">3</span>: env.action_space.sample(),\n        }\n    )\n\n    team0_reward += reward[<span class=\"hljs-number\">0</span>] + reward[<span class=\"hljs-number\">1</span>]\n    team1_reward += reward[<span class=\"hljs-number\">2</span>] + reward[<span class=\"hljs-number\">3</span>]\n    <span class=\"hljs-keyword\">if</span> done[<span class=\"hljs-string\">&quot;__all__&quot;</span>]:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Total Reward: &quot;</span>, team0_reward, <span class=\"hljs-string\">&quot; x &quot;</span>, team1_reward)\n        team0_reward = <span class=\"hljs-number\">0</span>\n        team1_reward = <span class=\"hljs-number\">0</span>\n        env.reset()\n</code></pre>\n<p>More information about the environment including reward functions and observation spaces can be found <a href=\"https://github.com/Unity-Technologies/ml-agents/blob/92ff2c26fef7174b443115454fa1c6045d622bc2/docs/Learning-Environment-Examples.md#soccer-twos\">here</a>.</p>\n<h3 id=\"watching--evaluating\">Watching / evaluating</h3>\n<p>You may implement your own rollout script using <code>soccer_twos.make(watch=True)</code> or use our CLI tool.\nTo rollout via CLI, you must create an implementation (subclass) of <code>soccer_twos.AgentInterface</code> and run <code>python -m soccer_twos.watch -m agent_module</code>.\nThis will run a human-friendly version of the environment, where your agent will play against itself.\nYou may instead run <code>python -m soccer_twos.watch -m1 agent_module -m2 opponent_module</code> to play against a different opponent.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\">\n</div>\n","readingTime":"4 min read","title":"Multiagent Soccer Environment for Python","slug":"2021-09-05-soccer-twos-env","date":"2021-09-05","urls":[{"cta":"Python Package","url":"https://pypi.org/project/ceia-soccer-twos/"},{"cta":"Code","url":"https://github.com/bryanoliveira/soccer-twos-env"}],"type":"Reinforcement Learning Environment","tags":["project","game","rl"],"image":"/img/soccer.gif","description":"A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil."},{"html":"<p>A <a href=\"https://en.wikipedia.org/wiki/Cellular_automaton\">Cellular Automata</a> program built with C++, OpenGL, CUDA and OpenMP. It&#39;s built to run on a GPU but it also supports multithreaded CPU-only execution. On the right there&#39;s an example execution of <a href=\"https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\">Conway&#39;s Game of Life</a> on a 100x100 randomly initialised lattice.</p>\n<p>The main objective of this project is to allow scaling up to a reasonably large number of cells while maintaining the code legibility and allowing for further customisations. It supports command-line arguments to set up quick configs (run <code>./automata -h</code> for details) like headless mode (which is significantly faster) and initial patterns (which can be loaded from the <code>patterns</code> folder). It doesn&#39;t yet support the definition of evolution rules at runtime or lattice size inference, but I&#39;m working on that.</p>\n<p>This program can currently evolve a dense &amp; high entropy 182.25 million cell Game of Life grid (13500x13500) with rendering enabled with up to 729 generations per second on a Ryzen 7 3700X / RTX 3080 using up to 200MB RAM and 8.5GB VRAM (which is the actual scaling limiter).</p>\n<p>The ability to evolve and render such large grids allows the program to run some really interesting patterns, like evolving the Game of Life <em>within</em> the Game of Life:</p>\n<div align=\"center\">\n    <img class=\"text-img mw-75\" src=\"https://github.com/bryanoliveira/cellular-automata/raw/master/images/zoom.gif\">\n</div>\n\n<p>In the GIF above we&#39;re running a 12300x12300 grid using Game of Life rules to evolve a pattern known as <a href=\"http://b3s23life.blogspot.com/2006_09_01_archive.html\">Meta-Toad</a>. It uses a grid of <a href=\"https://www.conwaylife.com/wiki/OTCA_metapixel\">OTCA Metapixels</a> and requires about 35 thousand generations of the underlying automaton to represent a single generation of the meta-grid. The pattern being evolved by the meta-grid is known as <a href=\"https://www.conwaylife.com/wiki/Toad\">Toad</a>:</p>\n<div align=\"center\">\n    <img class=\"text-img\" src=\"https://github.com/bryanoliveira/cellular-automata/raw/master/images/toad.gif\" width=\"150\">\n</div>\n\n<p>This program also supports a benchmark mode (<code>-b</code> option), which outputs the total and average evolution and rendering timings to stdout. Combined with <code>benchmark.sh</code> and <code>benchmark_visualize.ipynb</code>, it is possible to plot speedups and evolution times for different lattice sizes. Currently, the GPU implementation achieves a speedup up to 627x over the single-core CPU implementation.</p>\n<div align=\"center\">\n<br/>\n<img class=\"text-img mw-50\" src=\"https://raw.githubusercontent.com/bryanoliveira/cellular-automata/master/images/lat_hl_evo_speedup.png\">\n<img class=\"text-img mw-50\" src=\"https://raw.githubusercontent.com/bryanoliveira/cellular-automata/master/images/lat_hl_evo_avg.png\">\n</div>\n<br/>\n\n<blockquote>\n<p>Speedup over serial (left) and average grid evolution time in milliseconds (right) for lattice sizes 32x32, 64x64, ..., 4096x4096 and 1000 generations, using logarithmic X and Y axis. &quot;# Threads&quot; refers to the number of threads available for OpenMP CPU (Ryzen 7 3700X) runs while &quot;GPU&quot; refers to CUDA (RTX 3080) runs. For these tests, initial spawn probability was set to 0.5 and rendering was disabled.</p>\n</blockquote>\n<h2 id=\"requirements\">Requirements</h2>\n<p>To run the program you&#39;ll need:</p>\n<ul>\n<li>  Debian-like linux distro (I only tested this on Ubuntu 20)</li>\n<li>OpenGL* (GLEW, GLUT and GLM)<ul>\n<li>  e.g. <code>sudo apt-get install libglew-dev freeglut3-dev libglm-dev</code></li>\n</ul>\n</li>\n<li>  <a href=\"https://developer.nvidia.com/cuda-downloads\">CUDA</a>** (nvcc) and CUDA runtime libraries</li>\n</ul>\n<p>To build it from source you&#39;ll also need:</p>\n<ul>\n<li>g++ (C++ 17) and <em>make</em><ul>\n<li>  e.g. <code>sudo apt install build-essential</code></li>\n</ul>\n</li>\n<li>  Boost C++ Library (program_options module)</li>\n<li>  <a href=\"https://github.com/gabime/spdlog\">spdlog</a></li>\n</ul>\n<p>*It is possible to run this program in headless-only mode, so if your machine doesn&#39;t support rendering (e.g. Colab runtimes) you may skip the OpenGL installation step. For that to work you must compile the program with the <code>HEADLESS_ONLY</code> flag set (e.g. <code>make automata HEADLESS_ONLY=1</code>).</p>\n<p>**It is also possible to run this program in CPU-only mode, so if you don&#39;t have a CUDA-capable video card you may skip the CUDA installation step. For that to work you will need to compile the program with the <code>CPU_ONLY</code> flag set (e.g. <code>make automata CPU_ONLY=1</code>).</p>\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"building-from-source\">Building From Source</h3>\n<ul>\n<li>  Install the requirements</li>\n<li>  Clone this repository</li>\n<li>Building and executing:<ul>\n<li>  Run <code>make</code> to build and run</li>\n<li>  Run <code>make automata</code> to build</li>\n<li>  Run <code>make run</code> to run with default parameters</li>\n<li>  Run <code>make clean</code> to remove generated files</li>\n<li>  Run <code>make profile</code> to run <a href=\"https://developer.nvidia.com/nsight-systems\">NVIDIA&#39;s nsys</a> profiling.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"executing-a-pre-built-binary-linux-x64--cuda-only\">Executing a pre-built binary (Linux x64 + CUDA only)</h3>\n<ul>\n<li>  Download <code>cellular-automata-linux64.zip</code> from the <a href=\"https://github.com/bryanoliveira/cellular-automata/releases\">latest release</a></li>\n<li>  Extract the executable (<code>automata</code>) and the <code>patterns</code> folder</li>\n<li>  Install OpenGL and CUDA from the requirements above</li>\n<li>  Run <code>./automata -h</code> to see all the available options</li>\n<li>  Run the program with <code>./automata --render</code>.</li>\n</ul>\n<p>You may want to set the number of available threads when running in CPU. For that, set the environment variable <code>OMP_NUM_THREADS</code> (e.g. <code>env OMP_NUM_THREADS=8 ./automata -r</code>).</p>\n<p>If your GPU has enough VRAM (&gt;= 8 GB), you may be able to reproduce the Meta-Toad simulation above. Run <code>./automata -r -x 12300 -y 12300 -p 0 -f patterns/meta-toad.rle --skip-frames 80</code> to try it out!</p>\n<h3 id=\"runtime-controls\">Runtime Controls</h3>\n<ul>\n<li>Basic controls:<ul>\n<li>  <strong>space</strong> pauses/resumes the simulation;</li>\n<li>  <strong>enter/return</strong> runs a single generation;</li>\n<li>  <strong>left mouse click</strong> translates the grid relative to the max resolution</li>\n<li>  <strong>ctrl + left mouse click</strong> translates the camera relative to the world</li>\n<li>  <strong>mouse scroll</strong> zooms the grid in and out, relative to the max resolution</li>\n<li>  <strong>ctrl + mouse scroll</strong> zooms the camera, relative to the world</li>\n<li>  <strong>middle mouse click</strong> resets scale and translation</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"next-steps\">Next steps</h2>\n<p>There is still much room for improvement. This includes better memory management, use of CPU parallelism and automated tests. My next steps include (but are not limited to):</p>\n<ul>\n<li>  Addition of unit tests (in progress)</li>\n<li>  Usage of templates to abstract grid data types (e.g. cells should be represented with 1 bit instead of 8)</li>\n<li>  Usage of SM shared memory to explore data locality</li>\n<li>  Support for flexible rule definition</li>\n<li>  Support for infinite grids (e.g. storing only active cells)</li>\n<li>  Support for 3-D and N-D grids</li>\n</ul>\n<h2 id=\"references\">References</h2>\n<ul>\n<li>  What are <a href=\"https://en.wikipedia.org/wiki/Cellular_automaton\">Cellular Automata</a>?</li>\n<li>  What is <a href=\"https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\">Conway&#39;s Game of Life</a>?</li>\n<li>  <a href=\"http://golly.sourceforge.net/\">Golly</a>: an open source cellular automata simulator that supports several Game of Life and other automata algorithms;</li>\n<li>  <a href=\"https://copy.sh/life/\">Life</a>: an open source JavaScript implementation of Game of Life that runs in the browser;</li>\n<li>  <a href=\"http://b3s23life.blogspot.com/2006_09_01_archive.html\">Conway&#39;s Life: Work in Progress</a>: where I got the initial pattern for the Meta-Toad;</li>\n<li>  <a href=\"https://blog.amandaghassaei.com/2020/05/01/the-recursive-universe/\">The Recursive Universe</a>: explores and explains how some of the meta-patterns work;</li>\n<li>  What are <a href=\"https://www.conwaylife.com/wiki/OTCA_metapixel\">OTCA Metapixels</a>?</li>\n</ul>\n<h2 id=\"bonus\">Bonus</h2>\n<div align=\"center\">\n<img class=\"text-img mw-100\" src=\"https://github.com/bryanoliveira/cellular-automata/raw/master/images/1000x1000.gif\"/>\n</div>\n\n<blockquote>\n<p>A 1000x1000 randomly initialized grid running Game of life.</p>\n</blockquote>\n<hr>\n<p>This program was developed during the 2021/1 Parallel Computing (CCO0455) Computer Science graduate course at Universidade Federal de Goiás (UFG, Brazil).</p>\n","readingTime":"6 min read","title":"Cellular Automata Framework","slug":"2021-03-10-cellular-automata","date":"2021-03-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/cellular-automata"},{"cta":"Executable","url":"https://github.com/bryanoliveira/cellular-automata/releases"}],"type":"Project","tags":["project","parallel","cuda","opengl","game"],"image":"/img/cellular_automata.gif","description":"A <a href=\"https://en.wikipedia.org/wiki/Cellular_automaton\" target=\"_blank\">Cellular Automata</a> program built with C++, OpenGL, CUDA and OpenMP. The main objective of this project is to allow scaling up to a reasonably large number of cells while maintaining the code legibility and allowing for further customisations."},{"html":"<p>As my undergraduate thesis, I studied the impact of curiosity and intrinsic motivation as exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments. We found that this approach encourages increasing exploratory behaviours even after the goal tasks were learned. Furthermore, we found that adding information about other objects&#39; states into the agent&#39;s observation is crucial for learning complex behaviours when no dense reward signal is provided. This study was inspired by the <a href=\"https://www.aicrowd.com/challenges/robot-open-ended-autonomous-learning-real\">Robot open-Ended Autonomous Learning</a> competition.</p>\n<p>To read the full report, <a href=\"https://github.com/bryanlincoln/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf\">click here</a> (Portuguese).</p>\n<div align=\"center\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.gif\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.gif\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.gif\">\n</div>\n\n<blockquote>\n<p>Learned policies for the tasks Pick And Place (left), Push (center) and Reach (right).</p>\n</blockquote>\n<h2 id=\"requirements\">Requirements</h2>\n<ul>\n<li>  <a href=\"https://docs.python.org/\">Python 3</a></li>\n<li>  <a href=\"http://pytorch.org/\">PyTorch</a></li>\n<li>  <a href=\"https://github.com/openai/gym\">OpenAI Gym</a></li>\n<li>  <a href=\"https://github.com/openai/baselines\">OpenAI baselines</a></li>\n<li>  <a href=\"https://github.com/jmichaux/gym-fetch\">Gym Fetch</a></li>\n</ul>\n<h2 id=\"usage\">Usage</h2>\n<p>To run the code, simply execute <code>python main.py</code> after installing all the requirements. There are many customizable hyperparemeters and configurations. You can see them with <code>python main.py --help</code>. The exact hyperparameters for this study&#39;s experiments can be found in the folder <code>experiments</code>.</p>\n<h2 id=\"credits\">Credits</h2>\n<p>This code was based on and adapted from</p>\n<ul>\n<li>  <a href=\"https://github.com/jmichaux/intrinsic-motivation\">Jon Michaux&#39;s implementation of intrinsic motivation</a> (which was used as starting point for running the experiments of this repo)</li>\n<li>  <a href=\"https://github.com/srama2512/curiosity-driven-exploration\">Santhosh Ramakrishnan&#39;s implementation of curiosity</a></li>\n<li>  <a href=\"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\">Ilya Kostrikov&#39; implementation of recent RL algorithms</a></li>\n<li>  <a href=\"https://github.com/openai/baselines\">OpenAI Baselines</a></li>\n<li>  <a href=\"https://github.com/cbschaff/pytorch-dl\">Chip Schaff&#39;s Deep Learning Library</a></li>\n</ul>\n<p>The inspiration and theoretic background was mainly based on</p>\n<ul>\n<li>  <a href=\"https://pathak22.github.io/noreward-rl/\">Curiosity-driven Exploration by Self-supervised Prediction</a></li>\n<li>  <a href=\"https://pathak22.github.io/large-scale-curiosity/\">Large-Scale Study of Curiosity-Driven Learning</a></li>\n</ul>\n<h2 id=\"results\">Results</h2>\n<h3 id=\"success-rate-charts\">Success Rate Charts</h3>\n<p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.png\"> \n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.png\"> \n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.png\">\n</div>\n\n<h3 id=\"entropy-charts\"><a href=\"https://arxiv.org/abs/1811.11214\">Entropy</a> Charts</h3>\n<p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p>\n<div align=\"center\">\n<img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_ent.png\"> \n<img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_ent.png\"> \n<img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_ent.png\">\n</div>\n\n<h3 id=\"intrinsic-reward-charts\">Intrinsic Reward Charts</h3>\n<p>Pick And Place Task (left), Push Task (center) and Reach (right).</p>\n<div align=\"center\">\n<img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_int.png\"> \n<img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_int.png\"> \n<img class=\"text-img mw-33\" src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_int.png\">\n</div>\n\n<p>The interpretation of these curves can be found in my full report.</p>\n","readingTime":"2 min read","title":"Intrinsic motivation for robotic manipulation learning with sparse rewards","slug":"2019-12-10-intrinsic-motivation","date":"2019-12-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/intrinsic-motivation"},{"cta":"Paper","url":"https://github.com/bryanoliveira/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf"}],"type":"Undergraduate Thesis","tags":["research","publication"],"image":"/img/pick.gif","description":"Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."},{"html":"<p>This is my code for the <a href=\"https://www.kaggle.com/c/bone-age-regression\">I2A2 Bone Age Regression competition</a>. I learned a lot by building this pipeline from scratch and experimenting with different model architectures and optimizers. This was my first end-to-end image regression model, and it was very nice seeing my theoretical knowledge work in practice.</p>\n<p>This competition was inspired by <a href=\"https://www.kaggle.com/kmader/rsna-bone-age\">RSNA&#39;s Bone Age challenge</a>, in which given hand X-ray images, the model should predict the patient&#39;s bone age.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanoliveira/bone-age-regression/raw/master/docs/ex1.png\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanoliveira/bone-age-regression/raw/master/docs/ex2.png\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanoliveira/bone-age-regression/raw/master/docs/ex3.png\">\n</div>\n\n<blockquote>\n<p>X-ray images provided in the competition&#39;s dataset.</p>\n</blockquote>\n<p>My final solution used a <a href=\"https://arxiv.org/abs/1512.03385\">ResNet50</a> architecture, a <a href=\"https://arxiv.org/abs/1908.03265\">Rectified Adam</a> optimizer and geometric data augmentations. This model achieved a Mean Average Error of 13.2 after 20 epochs of training, which I believe could be improved given more training time and a better preprocessing pipeline (e.g. using object detection to segment the hands and normalizing hand rotation). Unfortunately, I didn&#39;t save all the hyperparameters I experimented with (neither their results), but you&#39;ll find the ones I used for my last submission in the code.</p>\n<p>I used <a href=\"https://www.tensorflow.org/tensorboard\">tensorboard</a> to log the training curves and <a href=\"https://github.com/tqdm/tqdm\">tqdm</a> to track progress. I also used <a href=\"https://github.com/bryanlincoln/fcm-notifier\">FCMNotifier</a>, a tool I made to send logs as notifications to my phone.</p>\n<h2 id=\"requirements\">Requirements</h2>\n<p>See <a href=\"https://github.com/bryanlincoln/bone-age-regression/blob/master/requirements.txt\">requirements.txt</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<ul>\n<li>  Download the requirements with <code>pip install -r requirements.txt</code></li>\n<li>  Download the dataset and sample submission with <code>sh download_data.sh</code>. You may need to log in with your Kaggle account in order to do it.</li>\n<li>  Train the ResNet50 model with <code>python boneage.py</code></li>\n<li>  Try different models and hyperparameters by editing the training script or use the <code>boneage.ipynb</code> notebook to do it interactively.</li>\n</ul>\n<h2 id=\"credits\">Credits</h2>\n<p>I used the vision models already <a href=\"https://github.com/pytorch/vision/tree/master/torchvision/models\">implemented in torchvision</a> with slight changes. You can try other torchvision models by adding the <code>in_channels</code> parameter to generalize the number of input channels since torchvision models work with RGB images.</p>\n","readingTime":"2 min read","title":"Bone Age Regression","slug":"2019-11-10-bone-age-regression","date":"2019-11-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/bone-age-regression"}],"type":"Deep Learning","tags":["project","ai","deeplearning"],"image":"/img/bone.png","description":"This is my code for the <a href=\"https://www.kaggle.com/c/bone-age-regression\">I2A2 Bone Age Regression competition</a>. I learned a lot by building this pipeline from scratch and experimenting with different model architectures and optimizers. This was my first end-to-end image regression model, and it was very nice seeing my theoretical knowledge work in practice."},{"html":"<p>Quack is a Unity3D game made for the Global Game Jam 2019 themed &quot;What home means to you?&quot; (<a href=\"https://globalgamejam.org/2019/games/quack\">check out the game&#39;s entry here</a>).\nThe game consists of a happy chicken that wants to build a new home for its children. You have to collect sticks and group them on top of the main tree to make a lovely nest. This game was developed within 12 hours.</p>\n<div align=\"center\">\n \n<img class=\"text-img mw-50\" src=\"https://ggj.s3.amazonaws.com/styles/game_sidebar__wide/featured_image/2019/01/263393/menu.png?itok=veRqhjix&timestamp=1548611925\"/>\n<img class=\"text-img mw-50\" src=\"https://ggj.s3.amazonaws.com/styles/feature_image__wide/games/screenshots/ingame3_9.png?itok=Xuf3MZan&timestamp=1548611547\"/>\n\n</div>\n\n<h2 id=\"diversifiers\">Diversifiers</h2>\n<ul>\n<li>  Language-Independence - (Sponsored by Valve Software) - Create a game that can be understood regardless of which language the player speaks</li>\n<li>  Keep it simple - Make your game playable by people who can use no more than a D-pad plus 2 buttons, with hardware like an Xbox Adaptive Controller in mind.</li>\n<li>  Assetless - Create all visuals programmatically or in the scene editor, and avoid any importing of image files, sprite sheets, 3D models etc.</li>\n<li>  Super Secret Stash - Feature a hidden room within your game.</li>\n</ul>\n<h2 id=\"credits\">Credits</h2>\n<p>This game was made by a group of 3 computer science students located in Goiânia, Brazil:</p>\n<ul>\n<li>  Bryan (me, <a href=\"mailto:&#x62;&#114;&#121;&#x61;&#110;&#x75;&#102;&#103;&#x40;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#x63;&#x6f;&#x6d;\">&#x62;&#114;&#121;&#x61;&#110;&#x75;&#102;&#103;&#x40;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#x63;&#x6f;&#x6d;</a>) programmed and modeled;</li>\n<li>  Luana (<a href=\"https://github.com/luanagbmartins\">@luanagbmartins</a> / <a href=\"mailto:&#x6c;&#x75;&#x61;&#110;&#x61;&#x67;&#98;&#109;&#x61;&#x72;&#116;&#105;&#110;&#115;&#x40;&#x67;&#109;&#x61;&#x69;&#108;&#46;&#99;&#111;&#109;\">&#x6c;&#x75;&#x61;&#110;&#x61;&#x67;&#98;&#109;&#x61;&#x72;&#116;&#105;&#110;&#115;&#x40;&#x67;&#109;&#x61;&#x69;&#108;&#46;&#99;&#111;&#109;</a>) programmed and designed the level;</li>\n<li>  Rennan (<a href=\"https://github.com/rennan11\">@rennan11</a> / <a href=\"mailto:&#x72;&#x65;&#x6e;&#110;&#x61;&#x6e;&#x65;&#x6c;&#105;&#x74;&#64;&#103;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d;\">&#x72;&#x65;&#x6e;&#110;&#x61;&#x6e;&#x65;&#x6c;&#105;&#x74;&#64;&#103;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d;</a>) designed the game and the sounds.</li>\n</ul>\n<p>In-game sound effects downloaded from Soundsnap.\nIn-game font &quot;Gloria Hallelujah&quot; by Kimberly Geswein, downloaded from Google Fonts.</p>\n","readingTime":"2 min read","title":"Quack","slug":"2019-01-10-quack","date":"2019-01-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/gjams-2019-quack"},{"cta":"Jam Entry","url":"https://globalgamejam.org/2019/games/quack"}],"type":"Game","tags":["project","game","jam","indie"],"image":"/img/quack.png","description":"Quack is a Unity3D game made for the Global Game Jam 2019 themed \"What home means to you?\". The game consists of a happy chicken that wants to build a new home for its children. You have to collect sticks and group them on top of the main tree to make a lovely nest. This game was developed within 12 hours."},{"html":"<p>3D Force simulator using only <a href=\"https://processing.org/\">Processing</a>&#39;s point() and line() functions. Uses Digital Differential Analyzer (DDA) to render lines between two points, Scan Line to render polygons, normal calculation to determine faces to render in 3D space and Newtonian physics. Written in Java.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"/img/rigidbody_physics.gif\"/>\n</div>\n\n<blockquote>\n<p>One of the project&#39;s scenes, where the cube is affected by gravity and the ground is not.</p>\n</blockquote>\n<p>The program interface allows for real-time selection, positioning, rotation, scaling and acceleration of objects. In a <a href=\"https://github.com/bryanoliveira/processing-3d-force-simulator/blob/fc899000baecf513cc3da4b38ab104cd4de260f7/Simulator/Projections.pde\">previous version</a> it also supported selecting between Cavalier, Cabinet, Isometric, Perspective-Z and Perspective-XZ projections.</p>\n<h2 id=\"usage\">Usage</h2>\n<ul>\n<li>  Clone this repository</li>\n<li>  Install <a href=\"https://processing.org/download/\">Processing</a></li>\n<li>  Open this project with Processing IDE or execute <code>processing-java --sketch=Simulator --force --run</code> in a CLI.</li>\n</ul>\n<hr>\n<p>This program was developed as the final project for the 2018/2 Computer Graphics (INF0037) class of Computer Science at Universidade Federal de Goiás (UFG, Brazil).</p>\n","readingTime":"1 min read","title":"3D Rendering & Force Simulator","slug":"2018-12-10-rigid-body-physics","date":"2018-12-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/processing-3d-force-simulator"}],"type":"Rendering","tags":["project","rendering","simulation"],"image":"/img/rigidbody_physics.gif","description":"3D Force simulator using only Processing's point() and line functions. Uses Digital Differential Analyzer (DDA) to render lines between two points, Scan Line to render polygons, normal calculation to determine faces to render in 3D space and Newtonian physics. Written in Java."},{"html":"<p>Hi! This is the development repo of <a href=\"https://www.facebook.com/NucleoPMec/\">Pequi Mecânico</a> - INF&#39;s <strong>Very Small Size Soccer Team</strong>. Our team comprises several courses (Electrical Engineering, Computer Engineering, Software Engineering and Computer Science), all from Federal University of Goiás - <a href=\"https://www.ufg.br/\">UFG</a> - Goiânia. Our repository is open because we understand that our greatest job is to add our research and knowledge to the academic and industrial world.</p>\n<p>You can find our Team Description Paper <a href=\"https://github.com/bryanoliveira/PY-VSSS-INF/blob/master/docs/TDP%20VSSS%20INF%202018.pdf\">here</a>. We are open to answer any questions and suggestions through our email <a href=\"mailto:&#112;&#x65;&#x71;&#x75;&#105;&#x6d;&#x65;&#x63;&#x61;&#110;&#105;&#99;&#x6f;&#117;&#102;&#103;&#x40;&#x67;&#x6d;&#97;&#105;&#108;&#46;&#x63;&#x6f;&#x6d;\">&#112;&#x65;&#x71;&#x75;&#105;&#x6d;&#x65;&#x63;&#x61;&#110;&#105;&#99;&#x6f;&#117;&#102;&#103;&#x40;&#x67;&#x6d;&#97;&#105;&#108;&#46;&#x63;&#x6f;&#x6d;</a>.</p>\n<h2 id=\"features\">Features</h2>\n<ul>\n<li>  Isolated modules for vision, strategy, control, communication, and interface</li>\n<li>  High-fidelity simulator made with MuJoCo</li>\n<li>  Qt interface</li>\n</ul>\n<div align=\"center\">\n    <a href=\"https://www.youtube.com/watch?v=JQVrX5h7u_8\">\n        <img class=\"text-img mw-100\" src=\"https://github.com/bryanoliveira/PY-VSSS-INF/raw/master/docs/images/Simulator.gif\"/>\n    </a>\n</div>\n\n<blockquote>\n<p>Our simulator running two instances of the same control system. Green arrows indicate the robot&#39;s movement direction (given by the vector field), and blue, yellow, and red arrows indicate goalie, defender and attacker&#39;s targets.</p>\n</blockquote>\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"dependencies\">Dependencies</h3>\n<p>Our code runs on all Python3-supported OS&#39;s (we recommend Python 3.4). To start using our software, you must install all requirements described on our requirements.txt file. You can easily do that by running:</p>\n<p><code>pip install -r requirements.txt</code></p>\n<p>We use MuJoCo as our simulation engine. Unfortunately, as MuJoCo is not free software, you must grab your license <a href=\"https://www.roboti.us/license.html\">here</a> to run our simulator.</p>\n<h3 id=\"executing\">Executing</h3>\n<p>To open the GUI we use on our competitions, run:</p>\n<p><code>python afrodite.py</code></p>\n<p>To open our simulator, run:</p>\n<p><code>python aether.py</code></p>\n<div align=\"center\">\n    <a href=\"https://www.youtube.com/watch?v=UBV4qlAJ-sc\">\n        <img class=\"text-img mw-100\" src=\"https://github.com/bryanoliveira/PY-VSSS-INF/raw/master/docs/images/Kick.gif\"/>\n    </a>\n</div>\n\n<h2 id=\"social-networks\">Social networks</h2>\n<p>Our activities and events always have updates. Follow us and get updated :D</p>\n<ul>\n<li>  <a href=\"https://www.instagram.com/pequimecanico/\">Instagram</a></li>\n<li>  <a href=\"https://www.facebook.com/NucleoPMec\">Facebook</a></li>\n</ul>\n","readingTime":"2 min read","title":"IEEE VSSS Team","slug":"2018-10-10-ieee-vsss-team","date":"2018-10-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/PY-VSSS-INF"},{"cta":"Simulator Demo","url":"https://www.youtube.com/watch?v=JQVrX5h7u_8"},{"cta":"Team Description Paper","url":"https://github.com/bryanoliveira/PY-VSSS-INF/blob/master/docs/TDP%20VSSS%20INF%202018.pdf"}],"type":"Robotics","tags":["project","robotics","leadership"],"image":"/img/vsss_cover.gif","description":"A stack consisting of image processing, computer vision, team coordination, navigation, control and communication software to compete in the 2018's Latin-American Robotics Competition for the Pequi Mecânico UFG - INF's team."},{"html":"<p>In a world overrun by zombies, you must survive as long as possible and defeat different bosses while unlocking weapons, upgrades, equipment, characters, maps and much more. Reviving the classics of the &#39;80s and &#39;90s, Die Zombit is a retro wave top-down shooting game with an amazing soundtrack and addictive gameplay that guarantees many hours of fun. Check out its trailer <a href=\"https://www.youtube.com/watch?v=sO7FSZ3TJns\">here</a>:</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"img/zombit_walter.gif\"/>\n</div>\n\n<p>I made this game during my first year in the Computer Science course, using C# and UnityScript. I&#39;ve learned a lot from this project, from software engineering to game design and a little bit of pixel art. However, I believe the project could be significantly improved using better design patterns, clearer abstractions and algorithmic complexity in mind. Yet, it is a fun game that you can <a href=\"https://play.google.com/store/apps/details?id=com.elitgames.zombit\">download</a> (an older version) in Play Store.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanoliveira/unity-zombit/raw/master/Images/3.png\"> \n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanoliveira/unity-zombit/raw/master/Images/2.png\"> \n    <img class=\"text-img mw-33\" src=\"https://github.com/bryanoliveira/unity-zombit/raw/master/Images/1.png\">\n</div>\n\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"running-a-pre-built-binary-android-and-windows-only\">Running a pre-built binary (Android and Windows only)</h3>\n<p>For the Android version, you may <a href=\"https://play.google.com/store/apps/details?id=com.elitgames.zombit\">head to the Play Store</a>.</p>\n<p>For the Windows version, do the following:</p>\n<ul>\n<li>Download Die.Zombit.[version].zip from the <a href=\"https://github.com/bryanoliveira/unity-zombit/releases\">latest release</a></li>\n<li>Extract the game folder</li>\n<li>Double click to run it.</li>\n</ul>\n<h3 id=\"buildingrunning-from-source\">Building/running from source</h3>\n<ul>\n<li>  Install Unity3D (tested for v2019.3)</li>\n<li>  Open the project folder</li>\n<li>  Hit Play and have fun!</li>\n</ul>\n<h2 id=\"next-steps\">Next steps</h2>\n<p>This version is an unfinished project that rethinks the UI, weapon management and overall objectives. It was made as an indie game event demo and some things are broken. The fixes and new features I plan to add include:</p>\n<ul>\n<li>  Fix joystick and touchscreen support</li>\n<li>  Fix pause menu UI</li>\n<li>  Fix end game UI</li>\n<li>  Add multiplayer support</li>\n</ul>\n<h2 id=\"more-gifs\">More GIFs</h2>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"img/zombit_monster.gif\"/><br/>\n    <img class=\"text-img mw-100\" src=\"img/zombit_round_up.gif\"/>\n</div>\n\n<h2 id=\"credits\">Credits</h2>\n<ul>\n<li>Soundtracks by<ul>\n<li>  <a href=\"https://soundcloud.com/bossfightswe\">Bossfight</a></li>\n<li>  <a href=\"https://soundcloud.com/ultrasyd\">Ultrasyd</a></li>\n<li>  <a href=\"https://soundcloud.com/dunderpatrullen\">Dunderpatrullen</a></li>\n<li>  <a href=\"https://soundcloud.com/detiouss\">Detious</a> &amp; <a href=\"https://soundcloud.com/lockyn\">Lockyn</a></li>\n</ul>\n</li>\n</ul>\n","readingTime":"2 min read","title":"Die Zombit","slug":"2015-06-10-die-zombit","date":"2015-06-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/unity-zombit"},{"cta":"Store","url":"https://play.google.com/store/apps/details?id=com.elitgames.zombit"},{"cta":"Trailer","url":"https://www.youtube.com/watch?v=sO7FSZ3TJns"}],"type":"Game","tags":["project","game","indie"],"image":"/img/zombit_cover.gif","description":"Reviving the classics of the 80's and 90's, Die Zombit is a retrowave top-down shooting game that has a striking soundtrack and an addictive gameplay which guarantee many hours of fun."}]