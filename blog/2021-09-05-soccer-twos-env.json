{"html":"<p>A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"/img/soccer.gif\">\n</div>\n\n<p>Pre-compiled versions of this environment are available for Linux, Windows and MacOS (x86, 64 bits). The source code for this environment is available <a href=\"https://github.com/bryanoliveira/unity-soccer\">here</a>.</p>\n<h2 id=\"requirements\">Requirements</h2>\n<p>See <a href=\"https://github.com/bryanoliveira/soccer-twos-env/blob/master/requirements.txt\">requirements.txt</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"for-training\">For training</h3>\n<p>Import this package and instantiate the environment:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n\nenv = soccer_twos.make()\n</code></pre>\n<p>The <code>make</code> method accepts several options:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>render</code></td>\n<td>Whether to render the environment. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>watch</code></td>\n<td>Whether to run an audience-friendly version the provided Soccer-Twos environment. Forces <code>render</code> to <code>True</code>, <code>time_scale</code> to <code>1</code> and <code>quality_level</code> to <code>5</code>. Has no effect when <code>env_path</code> is set. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>variation</code></td>\n<td>A soccer env variation in EnvType. Defaults to <code>EnvType.multiagent_player</code></td>\n</tr>\n<tr>\n<td><code>time_scale</code></td>\n<td>The time scale to use for the environment. This should be less than <code>100</code>x for better simulation accuracy. Defaults to <code>20</code>x realtime.</td>\n</tr>\n<tr>\n<td><code>quality_level</code></td>\n<td>The quality level to use when rendering the environment. Ranges between <code>0</code> (lowest) and <code>5</code> (highest). Defaults to <code>0</code>.</td>\n</tr>\n<tr>\n<td><code>base_port</code></td>\n<td>The base port to use to communicate with the environment. Defaults to <code>50039</code>.</td>\n</tr>\n<tr>\n<td><code>worker_id</code></td>\n<td>Used as base port shift to avoid communication conflicts. Defaults to <code>0</code>.</td>\n</tr>\n<tr>\n<td><code>env_path</code></td>\n<td>The path to the environment executable. Overrides <code>watch</code>. Defaults to the provided Soccer-Twos environment.</td>\n</tr>\n<tr>\n<td><code>flatten_branched</code></td>\n<td>If <code>True</code>, turn branched discrete action spaces into a <code>Discrete</code> space rather than <code>MultiDiscrete</code>. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>opponent_policy</code></td>\n<td>The policy to use for the opponent when <code>variation==team_vs_policy</code>. Defaults to a random agent.</td>\n</tr>\n<tr>\n<td><code>single_player</code></td>\n<td>Whether to let the agent control a single player, while the other stays still. Only works when <code>variation==team_vs_policy</code>. Defaults to <code>False</code>.</td>\n</tr>\n</tbody></table>\n<p>The created <code>env</code> exposes a basic <a href=\"https://gym.openai.com/\">Gym</a> interface.\nNamely, the methods <code>reset()</code>, <code>step(action: Dict[int, np.ndarray])</code> and <code>close()</code> are available.\nThe <code>render()</code> method has currently no effect and <code>soccer_twos.make(render=True)</code> should be used instead.\nThe <code>step()</code> method returns extra information about the player and the ball in the last tuple element.\nThis information may be used to build custom reward functions if needed.</p>\n<p>We expose an RLLib-compatible multiagent interface.\nThis means, for example, that <code>action</code> should be a <code>dict</code> where keys are integers in <code>{0, 1, 2, 3}</code> corresponding to each agent.\nAdditionally, values should be single actions shaped like <code>env.action_space.shape</code>.\nObservations and rewards follow the same structure. Dones are only set for the key <code>__all__</code>, which means &quot;all agents&quot;.\nAgents 0 and 1 correspond to the blue team and agents 2 and 3 correspond to the orange team.</p>\n<p>Here&#39;s a full example:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n\nenv = soccer_twos.make(render=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Observation Space: &quot;</span>, env.observation_space.shape)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Action Space: &quot;</span>, env.action_space.shape)\n\nteam0_reward = <span class=\"hljs-number\">0</span>\nteam1_reward = <span class=\"hljs-number\">0</span>\n<span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:\n    obs, reward, done, info = env.step(\n        {\n            <span class=\"hljs-number\">0</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">1</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">2</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">3</span>: env.action_space.sample(),\n        }\n    )\n\n    team0_reward += reward[<span class=\"hljs-number\">0</span>] + reward[<span class=\"hljs-number\">1</span>]\n    team1_reward += reward[<span class=\"hljs-number\">2</span>] + reward[<span class=\"hljs-number\">3</span>]\n    <span class=\"hljs-keyword\">if</span> done[<span class=\"hljs-string\">&quot;__all__&quot;</span>]:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Total Reward: &quot;</span>, team0_reward, <span class=\"hljs-string\">&quot; x &quot;</span>, team1_reward)\n        team0_reward = <span class=\"hljs-number\">0</span>\n        team1_reward = <span class=\"hljs-number\">0</span>\n        env.reset()\n</code></pre>\n<p>More information about the environment including reward functions and observation spaces can be found <a href=\"https://github.com/Unity-Technologies/ml-agents/blob/92ff2c26fef7174b443115454fa1c6045d622bc2/docs/Learning-Environment-Examples.md#soccer-twos\">here</a>.</p>\n<h3 id=\"watching--evaluating\">Watching / evaluating</h3>\n<p>You may implement your own rollout script using <code>soccer_twos.make(watch=True)</code> or use our CLI tool.\nTo rollout via CLI, you must create an implementation (subclass) of <code>soccer_twos.AgentInterface</code> and run <code>python -m soccer_twos.watch -m agent_module</code>.\nThis will run a human-friendly version of the environment, where your agent will play against itself.\nYou may instead run <code>python -m soccer_twos.watch -m1 agent_module -m2 opponent_module</code> to play against a different opponent.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\">\n</div>\n","readingTime":"4 min read","title":"Multiagent Soccer Environment for Python","slug":"2021-09-05-soccer-twos-env","date":"2021-09-05","urls":[{"cta":"Python Package","url":"https://pypi.org/project/ceia-soccer-twos/"},{"cta":"Code","url":"https://github.com/bryanoliveira/soccer-twos-env"}],"type":"Reinforcement Learning Environment","tags":["project","game","rl"],"image":"/img/soccer.gif","description":"A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil."}