{"html":"<p>A pre-compiled <a href=\"https://github.com/Unity-Technologies/ml-agents/blob/92ff2c26fef7174b443115454fa1c6045d622bc2/docs/Learning-Environment-Examples.md#soccer-twos\">Soccer-Twos</a> environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of <a href=\"https://github.com/Unity-Technologies/ml-agents\">Unity ML Agents</a> to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/soccer.gif\">\n</div>\n<br/>\n\n<p>Pre-compiled versions of this environment are available for Linux, Windows and MacOS (x86, 64 bits). The source code for this environment is available <a href=\"https://github.com/bryanoliveira/unity-soccer\">here</a>.</p>\n<h2 id=\"requirements\">Requirements</h2>\n<p>See <a href=\"https://github.com/bryanoliveira/soccer-twos-env/blob/master/requirements.txt\">requirements.txt</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"for-training\">For training</h3>\n<p>Import this package and instantiate the environment:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n\nenv = soccer_twos.make()\n</code></pre>\n<p>The <code>make</code> method accepts several options:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>render</code></td>\n<td>Whether to render the environment. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>watch</code></td>\n<td>Whether to run an audience-friendly version the provided Soccer-Twos environment. Forces <code>render</code> to <code>True</code>, <code>time_scale</code> to <code>1</code> and <code>quality_level</code> to <code>5</code>. Has no effect when <code>env_path</code> is set. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>variation</code></td>\n<td>A soccer env variation in EnvType. Defaults to <code>EnvType.multiagent_player</code></td>\n</tr>\n<tr>\n<td><code>blue_team_name</code></td>\n<td>The name of the blue team. Defaults to &quot;BLUE&quot;.</td>\n</tr>\n<tr>\n<td><code>orange_team_name</code></td>\n<td>The name of the orange team. Defaults to &quot;ORANGE&quot;.</td>\n</tr>\n<tr>\n<td><code>env_channel</code></td>\n<td>The side channel to use for communication with the environment. Defaults to None.</td>\n</tr>\n<tr>\n<td><code>time_scale</code></td>\n<td>The time scale to use for the environment. This should be less than <code>100</code>x for better simulation accuracy. Defaults to <code>20</code>x realtime.</td>\n</tr>\n<tr>\n<td><code>quality_level</code></td>\n<td>The quality level to use when rendering the environment. Ranges between <code>0</code> (lowest) and <code>5</code> (highest). Defaults to <code>0</code>.</td>\n</tr>\n<tr>\n<td><code>base_port</code></td>\n<td>The base port to use to communicate with the environment. Defaults to <code>50039</code>.</td>\n</tr>\n<tr>\n<td><code>worker_id</code></td>\n<td>Used as base port shift to avoid communication conflicts. Defaults to <code>0</code>.</td>\n</tr>\n<tr>\n<td><code>env_path</code></td>\n<td>The path to the environment executable. Overrides <code>watch</code>. Defaults to the provided Soccer-Twos environment.</td>\n</tr>\n<tr>\n<td><code>flatten_branched</code></td>\n<td>If <code>True</code>, turn branched discrete action spaces into a <code>Discrete</code> space rather than <code>MultiDiscrete</code>. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>opponent_policy</code></td>\n<td>The policy to use for the opponent when <code>variation==team_vs_policy</code>. Defaults to a random agent.</td>\n</tr>\n<tr>\n<td><code>single_player</code></td>\n<td>Whether to let the agent control a single player, while the other stays still. Only works when <code>variation==team_vs_policy</code>. Defaults to <code>False</code>.</td>\n</tr>\n</tbody></table>\n<p>The created <code>env</code> exposes a basic <a href=\"https://gym.openai.com/\">Gym</a> interface.\nNamely, the methods <code>reset()</code>, <code>step(action: Dict[int, np.ndarray])</code> and <code>close()</code> are available.\nThe <code>render()</code> method has currently no effect and <code>soccer_twos.make(render=True)</code> should be used instead.\nThe <code>step()</code> method returns extra information about the player and the ball in the last tuple element. This extra information includes position (x, y) and velocity (x, y) for the ball and players and y rotation (in degrees) of the players.</p>\n<p>We expose an RLLib-compatible multiagent interface.\nThis means, for example, that <code>action</code> should be a <code>dict</code> where keys are integers in <code>{0, 1, 2, 3}</code> corresponding to each agent.\nAdditionally, values should be single actions shaped like <code>env.action_space.shape</code>.\nObservations and rewards follow the same structure. Dones are only set for the key <code>__all__</code>, which means &quot;all agents&quot;.\nAgents 0 and 1 correspond to the blue team and agents 2 and 3 correspond to the orange team.</p>\n<p>Here&#39;s a full example:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n\nenv = soccer_twos.make(render=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Observation Space: &quot;</span>, env.observation_space.shape)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Action Space: &quot;</span>, env.action_space.shape)\n\nteam0_reward = <span class=\"hljs-number\">0</span>\nteam1_reward = <span class=\"hljs-number\">0</span>\n<span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\">True</span>:\n    obs, reward, done, info = env.step(\n        {\n            <span class=\"hljs-number\">0</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">1</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">2</span>: env.action_space.sample(),\n            <span class=\"hljs-number\">3</span>: env.action_space.sample(),\n        }\n    )\n\n    team0_reward += reward[<span class=\"hljs-number\">0</span>] + reward[<span class=\"hljs-number\">1</span>]\n    team1_reward += reward[<span class=\"hljs-number\">2</span>] + reward[<span class=\"hljs-number\">3</span>]\n    <span class=\"hljs-keyword\">if</span> done[<span class=\"hljs-string\">&quot;__all__&quot;</span>]:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;Total Reward: &quot;</span>, team0_reward, <span class=\"hljs-string\">&quot; x &quot;</span>, team1_reward)\n        team0_reward = <span class=\"hljs-number\">0</span>\n        team1_reward = <span class=\"hljs-number\">0</span>\n        env.reset()\n</code></pre>\n<h4 id=\"environment-state-configuration\">Environment State Configuration</h4>\n<p>The <code>env_channel</code> parameter allows for state configuration inside the simulation. To use it, you must first instantiate a <code>soccer_twos.side_channels.EnvConfigurationChannel</code> and pass it in the <code>soccer_twos.make</code> call. Here&#39;s a full example:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> soccer_twos\n<span class=\"hljs-keyword\">from</span> soccer_twos.side_channels <span class=\"hljs-keyword\">import</span> EnvConfigurationChannel\nenv_channel = EnvConfigurationChannel()\nenv = soccer_twos.make(env_channel=env_channel)\nenv.reset()\nenv_channel.set_parameters(\n    ball_state={\n        <span class=\"hljs-string\">&quot;position&quot;</span>: [<span class=\"hljs-number\">1</span>, -<span class=\"hljs-number\">1</span>],\n        <span class=\"hljs-string\">&quot;velocity&quot;</span>: [-<span class=\"hljs-number\">1.2</span>, <span class=\"hljs-number\">3</span>],\n    },\n    players_states={\n        <span class=\"hljs-number\">3</span>: {\n            <span class=\"hljs-string\">&quot;position&quot;</span>: [-<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">10</span>],\n            <span class=\"hljs-string\">&quot;rotation_y&quot;</span>: <span class=\"hljs-number\">45</span>,\n            <span class=\"hljs-string\">&quot;velocity&quot;</span>: [<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">0</span>],\n        }\n    }\n)\n<span class=\"hljs-comment\"># env.step()</span>\n</code></pre>\n<p>All the <code>env_channel.set_parameters</code> method parameters and dict keys are optional. You can set a single parameter at a time or the full game state if you need so.</p>\n<h3 id=\"evaluating\">Evaluating</h3>\n<p>To quickly evaluate one agent against another and generate comprehensive statistics, you may use the <code>evaluate</code> script:</p>\n<p><code>python -m soccer_twos.evaluate -m1 agent_module -m2 opponent_module</code></p>\n<p>You can also provide the <code>--episodes</code> option to specify the number of episodes to evaluate on (defaults to 100).</p>\n<h3 id=\"watching\">Watching</h3>\n<p>To rollout via CLI, you must create an implementation (subclass) of <code>soccer_twos.AgentInterface</code> and run:</p>\n<p><code>python -m soccer_twos.watch -m agent_module</code></p>\n<p>This will run a human-friendly version of the environment, where your agent will play against itself.\nYou may instead use the options <code>-m1 agent_module -m2 opponent_module</code> to play against a different opponent.\nYou may also implement your own rollout script using <code>soccer_twos.make(watch=True)</code>.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\"/>\n</div>\n\n<h2 id=\"environment-specs\">Environment Specs</h2>\n<p>This environment is based on Unity ML Agents&#39; <a href=\"https://github.com/Unity-Technologies/ml-agents/blob/92ff2c26fef7174b443115454fa1c6045d622bc2/docs/Learning-Environment-Examples.md#soccer-twos\">Soccer Twos</a>, so most of the specs are the same. Here, four agents compete in a 2 vs 2 toy soccer game, aiming to get the ball into the opponent&#39;s goal while preventing the ball from entering own goal.</p>\n<div align=\"center\">\n    <img class=\"text-img mw-100\" src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/obs.png\"/>\n</div>\n<br/>\n\n<ul>\n<li>  Observation space: a 336-dimensional vector corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees, each detecting 6 possible object types, along with the object&#39;s distance. The forward ray-casts contribute 264 state dimensions and backward 72 state dimensions over three observation stacks.</li>\n<li>  Action space: 3 discrete branched actions (MultiDiscrete) corresponding to forward, backward, sideways movement, as well as rotation (27 discrete actions).</li>\n<li>Agent Reward Function:<ul>\n<li>  <code>1 - accumulated time penalty</code>: when ball enters opponent&#39;s goal. Accumulated time penalty is incremented by <code>(1 / MaxSteps)</code> every fixed update and is reset to 0 at the beginning of an episode. In this build, <code>MaxSteps = 5000</code>.</li>\n<li>  <code>-1</code>: when ball enters team&#39;s goal.</li>\n</ul>\n</li>\n</ul>\n<p>Note that while this is true when <code>variation == EnvType.multiagent_player</code>, observation and action spaces may vary for other variations.</p>\n","readingTime":"6 min read","title":"Multiagent Soccer Environment for Python","slug":"2021-09-05-soccer-twos-env","date":"2021-09-05","urls":[{"cta":"Python Package","url":"https://pypi.org/project/ceia-soccer-twos/"},{"cta":"Environment Code","url":"https://github.com/bryanoliveira/soccer-twos-env"},{"cta":"Simulator Code","url":"https://github.com/bryanoliveira/unity-soccer"}],"type":"Reinforcement Learning Environment","tags":["project","game","rl"],"image":"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/soccer.gif","description":"A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil."}