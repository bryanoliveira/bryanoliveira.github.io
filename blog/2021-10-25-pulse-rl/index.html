<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"> <meta name=author content="Bryan Oliveira"> <meta name=theme-color content=#FFFFFF> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{html:"\u003Ch2 id=\"abstract\"\u003EAbstract\u003C\u002Fh2\u003E\n\u003Cp\u003EDigital Marketing Systems (DMS) are the primary point of contact between a digital business and its customers. In this context, the communication channel optimization problem poses a precious and still open challenge for DMS. Due to its interactive nature, Reinforcement Learning (RL) appears as a promising formulation for this problem. However, the standard RL setting learns from interacting with the environment, which is costly and dangerous for production systems. Furthermore, it also fails to learn from historical interactions due to the distributional shift between the collection and learning policies. For this matter, we present PulseRL, an offline RL-based production system for communication channel optimization built upon the Conservative Q-Learning (CQL) Framework. PulseRL architecture comprises the whole engineering pipeline (data processing, training, deployment, and monitoring), scaling to handle millions of users. Using CQL, PulseRL learns from historical logs, and its learning objective reduces the shift problem by mitigating the overestimation bias from out-of-distribution actions. We conducted experiments in a real-world DMS. Results show that PulseRL surpasses RL baselines with a significant margin in the online evaluation. They also validate the theoretical properties of CQL in a complex scenario with high sampling error and non-linear function approximation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"\u002Fimg\u002Fpulserl_architecture.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003EIllustration of PulseRL’s system pipeline. We compose it with different big data storage models, a data transformation engine, a task manager, and specialized microservices for training and inference, which ensures scalability for handling millions of users on a daily basis. It also provides version control for source code, dataset, MDP’s and RL agents.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EMore info soon.\u003C\u002Fp\u003E\n",readingTime:"2 min read",title:"PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning",slug:a,date:"2021-10-25",urls:[{cta:"Code",url:"https:\u002F\u002Fgithub.com\u002Fdlb-rl\u002Fpulse-rl"},{cta:"Paper",url:"https:\u002F\u002Foffline-rl-neurips.github.io\u002F2021\u002Fpdf\u002F9.pdf"},{cta:"Dataset",url:"https:\u002F\u002Fbit.ly\u002Fac-dataset"}],type:"Workshop Publication",tags:["research","publication"],image:"\u002Fimg\u002Fpulserl.png",description:"Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021)."},slug:a}}("2021-10-25-pulse-rl"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.5e4e0fb7.js"}catch(e){main="/client/legacy/client.83a618eb.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/[slug]-a70e5dcf.css rel=stylesheet> <title>PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning</title><link href=https://bryanoliveira.github.io/blog/2021-10-25-pulse-rl rel=canonical data-svelte=svelte-17660c7><meta data-svelte=svelte-17660c7 name=Description content="Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021)."><meta data-svelte=svelte-17660c7 content=article property=og:type><meta data-svelte=svelte-17660c7 content="PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning" property=og:title><meta data-svelte=svelte-17660c7 content=https://bryanoliveira.github.io/blog/2021-10-25-pulse-rl property=og:url><meta data-svelte=svelte-17660c7 content="Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021)." property=og:description><meta data-svelte=svelte-17660c7 content=/img/pulserl.png property=og:image><meta data-svelte=svelte-17660c7 name=image content=/img/pulserl.png><meta data-svelte=svelte-17660c7 name=twitter:card content=summary_large_image><meta data-svelte=svelte-17660c7 name=twitter:domain value=bryanoliveira.github.io><meta data-svelte=svelte-17660c7 name=twitter:creator value=bryanoliveira_><meta data-svelte=svelte-17660c7 name=twitter:title value="PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning"><meta data-svelte=svelte-17660c7 name=twitter:description content="Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021)."><meta data-svelte=svelte-17660c7 name=twitter:image content=/img/pulserl.png><meta data-svelte=svelte-17660c7 name=twitter:label1 value="Published on"><meta data-svelte=svelte-17660c7 name=twitter:data1 value="Oct 24, 2021"><meta data-svelte=svelte-17660c7 name=twitter:label2 value="Reading Time"><meta data-svelte=svelte-17660c7 name=twitter:data2 value="2 min read"> <link href=/client/client.5e4e0fb7.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/[slug].8d973d5b.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.5bcfa7e1.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/[slug]-a70e5dcf.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a class=back href=/blog rel=prefetch>« posts</a> <div class="text-center mb-5 mt-5"><h1>PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning</h1> <div class="text-muted mt-4">Workshop Publication · <span title=10/24/2021>October 2021</span> · 2 min read</div></div> <div class=row><div class=col-md-8><p>Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021).</p> <div class="text-center mb-3 text-md-left"><a class="no-underline btn btn-secondary btn-sm mr-1" href=https://github.com/dlb-rl/pulse-rl target=_blank>Code ⧉ </a><a class="no-underline btn btn-secondary btn-sm mr-1" href=https://offline-rl-neurips.github.io/2021/pdf/9.pdf target=_blank>Paper ⧉ </a><a class="no-underline btn btn-secondary btn-sm mr-1" href=https://bit.ly/ac-dataset target=_blank>Dataset ⧉ </a></div></div> <div class=col-md-4><img class="svelte-qbrnkn cover-img" src=/img/pulserl.png alt="PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning"></div></div> <hr> <div class="svelte-qbrnkn post"><h2 id=abstract>Abstract</h2> <p>Digital Marketing Systems (DMS) are the primary point of contact between a digital business and its customers. In this context, the communication channel optimization problem poses a precious and still open challenge for DMS. Due to its interactive nature, Reinforcement Learning (RL) appears as a promising formulation for this problem. However, the standard RL setting learns from interacting with the environment, which is costly and dangerous for production systems. Furthermore, it also fails to learn from historical interactions due to the distributional shift between the collection and learning policies. For this matter, we present PulseRL, an offline RL-based production system for communication channel optimization built upon the Conservative Q-Learning (CQL) Framework. PulseRL architecture comprises the whole engineering pipeline (data processing, training, deployment, and monitoring), scaling to handle millions of users. Using CQL, PulseRL learns from historical logs, and its learning objective reduces the shift problem by mitigating the overestimation bias from out-of-distribution actions. We conducted experiments in a real-world DMS. Results show that PulseRL surpasses RL baselines with a significant margin in the online evaluation. They also validate the theoretical properties of CQL in a complex scenario with high sampling error and non-linear function approximation.</p> <div align=center> <img class="mw-100 text-img" src=/img/pulserl_architecture.png> </div> <blockquote> <p>Illustration of PulseRL’s system pipeline. We compose it with different big data storage models, a data transformation engine, a task manager, and specialized microservices for training and inference, which ensures scalability for handling millions of users on a daily basis. It also provides version control for source code, dataset, MDP’s and RL agents.</p> </blockquote> <p><br><br></p> <p>More info soon.</p> </div> <hr> <footer class="text-center indicate_blank"><small class="svelte-1sdunwv horizontal-name-photo text-muted"><a class=no-underline href=.><img class=svelte-1sdunwv src=/img/me.jpg alt="Bryan Oliveira" id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small> </footer></main> </div></div> 