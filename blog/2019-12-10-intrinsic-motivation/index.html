<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"> <meta name=author content="Bryan Oliveira"> <meta name=theme-color content=#FFFFFF> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{html:"\u003Cp\u003EAs my undergraduate thesis, I studied the impact of curiosity and intrinsic motivation as exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments. We found that this approach encourages increasing exploratory behaviours even after the goal tasks were learned. Furthermore, we found that adding information about other objects&#39; states into the agent&#39;s observation is crucial for learning complex behaviours when no dense reward signal is provided. This study was inspired by the \u003Ca href=\"https:\u002F\u002Fwww.aicrowd.com\u002Fchallenges\u002Frobot-open-ended-autonomous-learning-real\"\u003ERobot open-Ended Autonomous Learning\u003C\u002Fa\u003E competition.\u003C\u002Fp\u003E\n\u003Cp\u003ETo read the full report, \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fblob\u002Fmaster\u002FText%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf\"\u003Eclick here\u003C\u002Fa\u003E (Portuguese).\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick.gif\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush.gif\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach.gif\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003ELearned policies for the tasks Pick And Place (left), Push (center) and Reach (right).\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Ch2 id=\"requirements\"\u003ERequirements\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fdocs.python.org\u002F\"\u003EPython 3\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"http:\u002F\u002Fpytorch.org\u002F\"\u003EPyTorch\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fopenai\u002Fgym\"\u003EOpenAI Gym\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fopenai\u002Fbaselines\"\u003EOpenAI baselines\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fjmichaux\u002Fgym-fetch\"\u003EGym Fetch\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Cp\u003ETo run the code, simply execute \u003Ccode\u003Epython main.py\u003C\u002Fcode\u003E after installing all the requirements. There are many customizable hyperparemeters and configurations. You can see them with \u003Ccode\u003Epython main.py --help\u003C\u002Fcode\u003E. The exact hyperparameters for this study&#39;s experiments can be found in the folder \u003Ccode\u003Eexperiments\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"credits\"\u003ECredits\u003C\u002Fh2\u003E\n\u003Cp\u003EThis code was based on and adapted from\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fjmichaux\u002Fintrinsic-motivation\"\u003EJon Michaux&#39;s implementation of intrinsic motivation\u003C\u002Fa\u003E (which was used as starting point for running the experiments of this repo)\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fsrama2512\u002Fcuriosity-driven-exploration\"\u003ESanthosh Ramakrishnan&#39;s implementation of curiosity\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fikostrikov\u002Fpytorch-a2c-ppo-acktr-gail\"\u003EIlya Kostrikov&#39; implementation of recent RL algorithms\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fopenai\u002Fbaselines\"\u003EOpenAI Baselines\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fcbschaff\u002Fpytorch-dl\"\u003EChip Schaff&#39;s Deep Learning Library\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EThe inspiration and theoretic background was mainly based on\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fpathak22.github.io\u002Fnoreward-rl\u002F\"\u003ECuriosity-driven Exploration by Self-supervised Prediction\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fpathak22.github.io\u002Flarge-scale-curiosity\u002F\"\u003ELarge-Scale Study of Curiosity-Driven Learning\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"results\"\u003EResults\u003C\u002Fh2\u003E\n\u003Ch3 id=\"success-rate-charts\"\u003ESuccess Rate Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch3 id=\"entropy-charts\"\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1811.11214\"\u003EEntropy\u003C\u002Fa\u003E Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick_ent.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush_ent.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach_ent.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch3 id=\"intrinsic-reward-charts\"\u003EIntrinsic Reward Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right).\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick_int.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush_int.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach_int.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003EThe interpretation of these curves can be found in my full report.\u003C\u002Fp\u003E\n",readingTime:"2 min read",title:"Intrinsic motivation for robotic manipulation learning with sparse rewards",slug:a,date:"2019-12-10",urls:[{cta:"Code",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fintrinsic-motivation"},{cta:"Paper",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fundergraduate-thesis\u002Fblob\u002Fmaster\u002FText%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf"}],type:"Undergraduate Thesis",tags:["research","publication"],image:"\u002Fimg\u002Fpick.gif",description:"Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."},slug:a}}("2019-12-10-intrinsic-motivation"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.a1e9e4ee.js"}catch(e){main="/client/legacy/client.cf532d86.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/[slug]-a70e5dcf.css rel=stylesheet> <title>Intrinsic motivation for robotic manipulation learning with sparse rewards</title><link href=https://bryanoliveira.github.io/blog/2019-12-10-intrinsic-motivation rel=canonical data-svelte=svelte-15p38pq><meta data-svelte=svelte-15p38pq name=Description content="Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."><meta data-svelte=svelte-15p38pq content=article property=og:type><meta data-svelte=svelte-15p38pq content="Intrinsic motivation for robotic manipulation learning with sparse rewards" property=og:title><meta data-svelte=svelte-15p38pq content=https://bryanoliveira.github.io/blog/2019-12-10-intrinsic-motivation property=og:url><meta data-svelte=svelte-15p38pq content="Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments." property=og:description><meta data-svelte=svelte-15p38pq name=image content=/img/pick.gif property=og:image><meta data-svelte=svelte-15p38pq name=twitter:card content=summary_large_image><meta data-svelte=svelte-15p38pq name=twitter:domain value=bryanoliveira.github.io><meta data-svelte=svelte-15p38pq name=twitter:creator value=bryanoliveira_><meta data-svelte=svelte-15p38pq name=twitter:title value="Intrinsic motivation for robotic manipulation learning with sparse rewards"><meta data-svelte=svelte-15p38pq name=twitter:description content="Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."><meta data-svelte=svelte-15p38pq name=twitter:image content=/img/pick.gif><meta data-svelte=svelte-15p38pq name=twitter:label1 value="Published on"><meta data-svelte=svelte-15p38pq name=twitter:data1 value="Dec 9, 2019"><meta data-svelte=svelte-15p38pq name=twitter:label2 value="Reading Time"><meta data-svelte=svelte-15p38pq name=twitter:data2 value="2 min read"> <link href=/client/client.a1e9e4ee.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/[slug].cbe3d43d.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.a0020f40.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/[slug]-a70e5dcf.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a href=/blog class=back rel=prefetch>« posts</a> <div class="text-center mb-5 mt-5"><h1>Intrinsic motivation for robotic manipulation learning with sparse rewards</h1> <div class="text-muted mt-4">Undergraduate Thesis · <span title=12/9/2019>December 2019</span> · 2 min read</div></div> <div class=row><div class=col-md-8><p>Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments.</p> <div class="text-center mb-3 text-md-left"><a href=https://github.com/bryanoliveira/intrinsic-motivation class="no-underline btn btn-secondary btn-sm mr-1" target=_blank>Code ⧉ </a><a href=https://github.com/bryanoliveira/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf class="no-underline btn btn-secondary btn-sm mr-1" target=_blank>Paper ⧉ </a></div></div> <div class=col-md-4><img class="svelte-qbrnkn cover-img" src=/img/pick.gif alt="Intrinsic motivation for robotic manipulation learning with sparse rewards"></div></div> <hr> <div class="svelte-qbrnkn post"><p>As my undergraduate thesis, I studied the impact of curiosity and intrinsic motivation as exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments. We found that this approach encourages increasing exploratory behaviours even after the goal tasks were learned. Furthermore, we found that adding information about other objects' states into the agent's observation is crucial for learning complex behaviours when no dense reward signal is provided. This study was inspired by the <a href=https://www.aicrowd.com/challenges/robot-open-ended-autonomous-learning-real>Robot open-Ended Autonomous Learning</a> competition.</p> <p>To read the full report, <a href=https://github.com/bryanlincoln/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf>click here</a> (Portuguese).</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.gif> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.gif> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.gif> </div> <blockquote> <p>Learned policies for the tasks Pick And Place (left), Push (center) and Reach (right).</p> </blockquote> <h2 id=requirements>Requirements</h2> <ul> <li> <a href=https://docs.python.org/ >Python 3</a></li> <li> <a href=http://pytorch.org/ >PyTorch</a></li> <li> <a href=https://github.com/openai/gym>OpenAI Gym</a></li> <li> <a href=https://github.com/openai/baselines>OpenAI baselines</a></li> <li> <a href=https://github.com/jmichaux/gym-fetch>Gym Fetch</a></li> </ul> <h2 id=usage>Usage</h2> <p>To run the code, simply execute <code>python main.py</code> after installing all the requirements. There are many customizable hyperparemeters and configurations. You can see them with <code>python main.py --help</code>. The exact hyperparameters for this study's experiments can be found in the folder <code>experiments</code>.</p> <h2 id=credits>Credits</h2> <p>This code was based on and adapted from</p> <ul> <li> <a href=https://github.com/jmichaux/intrinsic-motivation>Jon Michaux's implementation of intrinsic motivation</a> (which was used as starting point for running the experiments of this repo)</li> <li> <a href=https://github.com/srama2512/curiosity-driven-exploration>Santhosh Ramakrishnan's implementation of curiosity</a></li> <li> <a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail>Ilya Kostrikov' implementation of recent RL algorithms</a></li> <li> <a href=https://github.com/openai/baselines>OpenAI Baselines</a></li> <li> <a href=https://github.com/cbschaff/pytorch-dl>Chip Schaff's Deep Learning Library</a></li> </ul> <p>The inspiration and theoretic background was mainly based on</p> <ul> <li> <a href=https://pathak22.github.io/noreward-rl/ >Curiosity-driven Exploration by Self-supervised Prediction</a></li> <li> <a href=https://pathak22.github.io/large-scale-curiosity/ >Large-Scale Study of Curiosity-Driven Learning</a></li> </ul> <h2 id=results>Results</h2> <h3 id=success-rate-charts>Success Rate Charts</h3> <p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.png> </div> <h3 id=entropy-charts><a href=https://arxiv.org/abs/1811.11214>Entropy</a> Charts</h3> <p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_ent.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_ent.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_ent.png> </div> <h3 id=intrinsic-reward-charts>Intrinsic Reward Charts</h3> <p>Pick And Place Task (left), Push Task (center) and Reach (right).</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_int.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_int.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_int.png> </div> <p>The interpretation of these curves can be found in my full report.</p> </div> <hr> <footer class="text-center indicate_blank"><small class="svelte-1sdunwv horizontal-name-photo text-muted"><a href=. class=no-underline><img class=svelte-1sdunwv src=/img/me.jpg alt="Bryan Oliveira" id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small> </footer></main> </div></div> 