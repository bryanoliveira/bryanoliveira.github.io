<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"> <meta name=author content="Bryan Oliveira"> <meta name=theme-color content=#FFFFFF> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{html:"\u003Cp\u003EMachine Learning Algorithms have become increasingly efficient at solving complexreal-world problems. In particular, Reinforcement Learning algorithms are capable of learning behaviors applicable to robotics that can replace or work together with classical control models, thereby increasing their robustness, applicability and viability. However,it remains difficult to design reward functions that represent, for a reinforcement learning agent, the task it must perform. Recent research in this area proposes techniques such as curiosity and intrinsic motivation as an alternative to the use of extrinsic environmental rewards, proving to be efficient in guiding the agent to satisfactory exploration in game environments such as VizDoom and Super Mario Bros. This paper analyzes the impact of the intrinsic motivation technique on agent training in robotic simulation environments, as well as its general implications for aspects such as generalization, exploration and sampling efficiency. We found that this approach encourages increasing exploratory behaviors even after the goal tasks were learned. Furthermore, we found that adding information about other objects&#39; states into the agent&#39;s observation is crucial for learning complex behaviors when no dense reward signal is provided. This, however, requires the agent to learn it&#39;s own dynamics before interacting with the rest of the environment.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick.gif\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush.gif\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach.gif\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003ELearned policies for the tasks Pick And Place (left), Push (center) and Reach (right).\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003ETo read the full report, \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fblob\u002Fmaster\u002FMonografia.pdf\"\u003Eclick here\u003C\u002Fa\u003E (Portuguese).\u003C\u002Fp\u003E\n\u003Cp\u003EThis study was inspired by the \u003Ca href=\"https:\u002F\u002Fwww.aicrowd.com\u002Fchallenges\u002Frobot-open-ended-autonomous-learning-real\"\u003ERobot open-Ended Autonomous Learning\u003C\u002Fa\u003E competition. The theoretic background was mainly based on\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fpathak22.github.io\u002Fnoreward-rl\u002F\"\u003ECuriosity-driven Exploration by Self-supervised Prediction\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fpathak22.github.io\u002Flarge-scale-curiosity\u002F\"\u003ELarge-Scale Study of Curiosity-Driven Learning\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"success-rate-charts\"\u003ESuccess Rate Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch3 id=\"entropy-charts\"\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1811.11214\"\u003EEntropy\u003C\u002Fa\u003E Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick_ent.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush_ent.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach_ent.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch3 id=\"intrinsic-reward-charts\"\u003EIntrinsic Reward Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right).\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick_int.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush_int.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach_int.png\"\u003E\n\u003C\u002Fdiv\u003E\n",readingTime:"2 min read",title:"Intrinsic motivation for robotic manipulation learning with sparse rewards",slug:a,date:"2019-12-10",urls:[{cta:"Paper",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fundergraduate-thesis\u002Fblob\u002Fmaster\u002FText%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf"}],type:"Undergraduate Thesis",tags:["research","publication"],image:"\u002Fimg\u002Fpick.gif",description:"Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."},slug:a}}("2019-12-10-intrinsic-motivation"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.2f00842c.js"}catch(e){main="/client/legacy/client.aac98c50.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/[slug]-a70e5dcf.css rel=stylesheet> <title>Intrinsic motivation for robotic manipulation learning with sparse rewards</title><link href=https://bryanoliveira.github.io/blog/2019-12-10-intrinsic-motivation rel=canonical data-svelte=svelte-17660c7><meta data-svelte=svelte-17660c7 name=Description content="Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."><meta data-svelte=svelte-17660c7 content=article property=og:type><meta data-svelte=svelte-17660c7 content="Intrinsic motivation for robotic manipulation learning with sparse rewards" property=og:title><meta data-svelte=svelte-17660c7 content=https://bryanoliveira.github.io/blog/2019-12-10-intrinsic-motivation property=og:url><meta data-svelte=svelte-17660c7 content="Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments." property=og:description><meta data-svelte=svelte-17660c7 content=/img/pick.gif property=og:image><meta data-svelte=svelte-17660c7 name=image content=/img/pick.gif><meta data-svelte=svelte-17660c7 name=twitter:card content=summary_large_image><meta data-svelte=svelte-17660c7 name=twitter:domain value=bryanoliveira.github.io><meta data-svelte=svelte-17660c7 name=twitter:creator value=bryanoliveira_><meta data-svelte=svelte-17660c7 name=twitter:title value="Intrinsic motivation for robotic manipulation learning with sparse rewards"><meta data-svelte=svelte-17660c7 name=twitter:description content="Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."><meta data-svelte=svelte-17660c7 name=twitter:image content=/img/pick.gif><meta data-svelte=svelte-17660c7 name=twitter:label1 value="Published on"><meta data-svelte=svelte-17660c7 name=twitter:data1 value="Dec 10, 2019"><meta data-svelte=svelte-17660c7 name=twitter:label2 value="Reading Time"><meta data-svelte=svelte-17660c7 name=twitter:data2 value="2 min read"> <link href=/client/client.2f00842c.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/[slug].a8ebadb5.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.87fc8019.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/[slug]-a70e5dcf.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a href=/blog class=back rel=prefetch>« posts</a> <div class="text-center mb-5 mt-5"><h1>Intrinsic motivation for robotic manipulation learning with sparse rewards</h1> <div class="text-muted mt-4">Undergraduate Thesis · <span title=12/10/2019>December 2019</span> · 2 min read</div></div> <div class=row><div class=col-md-8><p>Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments.</p> <div class="text-center mb-3 text-md-left"><a href=https://github.com/bryanoliveira/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf class="no-underline btn btn-secondary btn-sm mr-1" target=_blank>Paper ⧉ </a></div></div> <div class=col-md-4><img class="svelte-qbrnkn cover-img" src=/img/pick.gif alt="Intrinsic motivation for robotic manipulation learning with sparse rewards"></div></div> <hr> <div class="svelte-qbrnkn post"><p>Machine Learning Algorithms have become increasingly efficient at solving complexreal-world problems. In particular, Reinforcement Learning algorithms are capable of learning behaviors applicable to robotics that can replace or work together with classical control models, thereby increasing their robustness, applicability and viability. However,it remains difficult to design reward functions that represent, for a reinforcement learning agent, the task it must perform. Recent research in this area proposes techniques such as curiosity and intrinsic motivation as an alternative to the use of extrinsic environmental rewards, proving to be efficient in guiding the agent to satisfactory exploration in game environments such as VizDoom and Super Mario Bros. This paper analyzes the impact of the intrinsic motivation technique on agent training in robotic simulation environments, as well as its general implications for aspects such as generalization, exploration and sampling efficiency. We found that this approach encourages increasing exploratory behaviors even after the goal tasks were learned. Furthermore, we found that adding information about other objects' states into the agent's observation is crucial for learning complex behaviors when no dense reward signal is provided. This, however, requires the agent to learn it's own dynamics before interacting with the rest of the environment.</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.gif> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.gif> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.gif> </div> <blockquote> <p>Learned policies for the tasks Pick And Place (left), Push (center) and Reach (right).</p> </blockquote> <p>To read the full report, <a href=https://github.com/bryanlincoln/undergraduate-thesis/blob/master/Monografia.pdf>click here</a> (Portuguese).</p> <p>This study was inspired by the <a href=https://www.aicrowd.com/challenges/robot-open-ended-autonomous-learning-real>Robot open-Ended Autonomous Learning</a> competition. The theoretic background was mainly based on</p> <ul> <li> <a href=https://pathak22.github.io/noreward-rl/ >Curiosity-driven Exploration by Self-supervised Prediction</a></li> <li> <a href=https://pathak22.github.io/large-scale-curiosity/ >Large-Scale Study of Curiosity-Driven Learning</a></li> </ul> <h3 id=success-rate-charts>Success Rate Charts</h3> <p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.png> </div> <h3 id=entropy-charts><a href=https://arxiv.org/abs/1811.11214>Entropy</a> Charts</h3> <p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_ent.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_ent.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_ent.png> </div> <h3 id=intrinsic-reward-charts>Intrinsic Reward Charts</h3> <p>Pick And Place Task (left), Push Task (center) and Reach (right).</p> <div align=center> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_int.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_int.png> <img class="mw-33 text-img" src=https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_int.png> </div> </div> <hr> <footer class="text-center indicate_blank"><small class="svelte-1sdunwv horizontal-name-photo text-muted"><a href=. class=no-underline><img class=svelte-1sdunwv src=/img/me.jpg alt="Bryan Oliveira" id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small> </footer></main> </div></div> 