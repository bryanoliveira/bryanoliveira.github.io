<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1,shrink-to-fit=no" name=viewport> <meta content="Bryan Oliveira" name=author> <meta content=#FFFFFF name=theme-color> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,(function(a,b,c,d,e,f,g,h,i,j,k){return {posts:[{html:"\u003Cp\u003EThe Debt Collection (RC) market presents vast potential for enhancing its processes through the implementation of Machine Learning (AM) techniques, due to the profusion of data accumulated by debt collection institutions. This Systematic Literature Review aims to identify techniques applied to the specific predicament of Debt Pricing (PD), especially Reinforcement Learning, and to highlight research opportunities that remain unexplored. A research methodology is instituted, and statistics of the findings are presented, as well as a synthesis of the main methods identified. It is concluded that the optimization of PD is still an open question and that the use of AM in RC still lacks comprehensive studies. Furthermore, the reviewed works evidenced a deficiency in reproducibility and comparability of the results obtained.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"\u002Fimg\u002Fciclo_do_credito.jpg\"\u003E\n\u003C\u002Fdiv\u003E\n",readingTime:g,title:"Data-Driven Debt Pricing: A Systematic Literature Review",slug:"2023-02-debt-pricing-slr",date:"2023-02-19",urls:[{cta:d,url:"\u002Fpdfs\u002FPrecificacao_de_Dividas_Orientada_a_Dados__Revisao_Sistematica_da_Literatura.pdf"}],type:"Research Paper",tags:[f],image:"\u002Fimg\u002Fdeal.png",description:"This review explores the potential of machine learning in debt pricing, with a focus on reinforcement learning. It concludes that more research is needed and highlights issues with reproducibility and comparability of results."},{html:"\u003Ch2 id=\"abstract\"\u003EAbstract\u003C\u002Fh2\u003E\n\u003Cp\u003EDigital Marketing Systems (DMS) are the primary point of contact between a digital business and its customers. In this context, the communication channel optimization problem poses a precious and still open challenge for DMS. Due to its interactive nature, Reinforcement Learning (RL) appears as a promising formulation for this problem. However, the standard RL setting learns from interacting with the environment, which is costly and dangerous for production systems. Furthermore, it also fails to learn from historical interactions due to the distributional shift between the collection and learning policies. For this matter, we present PulseRL, an offline RL-based production system for communication channel optimization built upon the Conservative Q-Learning (CQL) Framework. PulseRL architecture comprises the whole engineering pipeline (data processing, training, deployment, and monitoring), scaling to handle millions of users. Using CQL, PulseRL learns from historical logs, and its learning objective reduces the shift problem by mitigating the overestimation bias from out-of-distribution actions. We conducted experiments in a real-world DMS. Results show that PulseRL surpasses RL baselines with a significant margin in the online evaluation. They also validate the theoretical properties of CQL in a complex scenario with high sampling error and non-linear function approximation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"\u002Fimg\u002Fpulserl_architecture.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003EIllustration of PulseRL’s system pipeline. We compose it with different big data storage models, a data transformation engine, a task manager, and specialized microservices for training and inference, which ensures scalability for handling millions of users on a daily basis. It also provides version control for source code, dataset, MDP’s and RL agents.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EMore info soon.\u003C\u002Fp\u003E\n",readingTime:c,title:"PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning",slug:"2021-10-25-pulse-rl",date:"2021-10-25",urls:[{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fdlb-rl\u002Fpulse-rl"},{cta:d,url:"https:\u002F\u002Foffline-rl-neurips.github.io\u002F2021\u002Fpdf\u002F9.pdf"},{cta:"Dataset",url:"https:\u002F\u002Fbit.ly\u002Fac-dataset"}],type:"Workshop Publication",tags:[f,h],image:"\u002Fimg\u002Fpulserl.png",description:"Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021)."},{html:"\u003Cp\u003EA pre-compiled \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FUnity-Technologies\u002Fml-agents\u002Fblob\u002F92ff2c26fef7174b443115454fa1c6045d622bc2\u002Fdocs\u002FLearning-Environment-Examples.md#soccer-twos\"\u003ESoccer-Twos\u003C\u002Fa\u003E environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FUnity-Technologies\u002Fml-agents\"\u003EUnity ML Agents\u003C\u002Fa\u003E to be used as final assignment for the Reinforcement Learning Minicourse at CEIA \u002F Deep Learning Brazil.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"\u002Fimg\u002Fsoccer.gif\"\u003E\n\u003C\u002Fdiv\u003E\n\u003Cbr\u002F\u003E\n\n\u003Cp\u003EPre-compiled versions of this environment are available for Linux, Windows and MacOS (x86, 64 bits). The source code for this environment is available \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-soccer\"\u003Ehere\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"requirements\"\u003ERequirements\u003C\u002Fh2\u003E\n\u003Cp\u003ESee \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fsoccer-twos-env\u002Fblob\u002Fmaster\u002Frequirements.txt\"\u003Erequirements.txt\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Ch3 id=\"for-training\"\u003EFor training\u003C\u002Fh3\u003E\n\u003Cp\u003EImport this package and instantiate the environment:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E soccer_twos\n\nenv = soccer_twos.make()\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThe \u003Ccode\u003Emake\u003C\u002Fcode\u003E method accepts several options:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EOption\u003C\u002Fth\u003E\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Erender\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EWhether to render the environment. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ewatch\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EWhether to run an audience-friendly version the provided Soccer-Twos environment. Forces \u003Ccode\u003Erender\u003C\u002Fcode\u003E to \u003Ccode\u003ETrue\u003C\u002Fcode\u003E, \u003Ccode\u003Etime_scale\u003C\u002Fcode\u003E to \u003Ccode\u003E1\u003C\u002Fcode\u003E and \u003Ccode\u003Equality_level\u003C\u002Fcode\u003E to \u003Ccode\u003E5\u003C\u002Fcode\u003E. Has no effect when \u003Ccode\u003Eenv_path\u003C\u002Fcode\u003E is set. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Evariation\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EA soccer env variation in EnvType. Defaults to \u003Ccode\u003EEnvType.multiagent_player\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eblue_team_name\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe name of the blue team. Defaults to &quot;BLUE&quot;.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eorange_team_name\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe name of the orange team. Defaults to &quot;ORANGE&quot;.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eenv_channel\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe side channel to use for communication with the environment. Defaults to None.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Etime_scale\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe time scale to use for the environment. This should be less than \u003Ccode\u003E100\u003C\u002Fcode\u003Ex for better simulation accuracy. Defaults to \u003Ccode\u003E20\u003C\u002Fcode\u003Ex realtime.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Equality_level\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe quality level to use when rendering the environment. Ranges between \u003Ccode\u003E0\u003C\u002Fcode\u003E (lowest) and \u003Ccode\u003E5\u003C\u002Fcode\u003E (highest). Defaults to \u003Ccode\u003E0\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ebase_port\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe base port to use to communicate with the environment. Defaults to \u003Ccode\u003E50039\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eworker_id\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EUsed as base port shift to avoid communication conflicts. Defaults to \u003Ccode\u003E0\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eenv_path\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe path to the environment executable. Overrides \u003Ccode\u003Ewatch\u003C\u002Fcode\u003E. Defaults to the provided Soccer-Twos environment.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eflatten_branched\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EIf \u003Ccode\u003ETrue\u003C\u002Fcode\u003E, turn branched discrete action spaces into a \u003Ccode\u003EDiscrete\u003C\u002Fcode\u003E space rather than \u003Ccode\u003EMultiDiscrete\u003C\u002Fcode\u003E. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eopponent_policy\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe policy to use for the opponent when \u003Ccode\u003Evariation==team_vs_policy\u003C\u002Fcode\u003E. Defaults to a random agent.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Esingle_player\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EWhether to let the agent control a single player, while the other stays still. Only works when \u003Ccode\u003Evariation==team_vs_policy\u003C\u002Fcode\u003E. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003EThe created \u003Ccode\u003Eenv\u003C\u002Fcode\u003E exposes a basic \u003Ca href=\"https:\u002F\u002Fgym.openai.com\u002F\"\u003EGym\u003C\u002Fa\u003E interface.\nNamely, the methods \u003Ccode\u003Ereset()\u003C\u002Fcode\u003E, \u003Ccode\u003Estep(action: Dict[int, np.ndarray])\u003C\u002Fcode\u003E and \u003Ccode\u003Eclose()\u003C\u002Fcode\u003E are available.\nThe \u003Ccode\u003Erender()\u003C\u002Fcode\u003E method has currently no effect and \u003Ccode\u003Esoccer_twos.make(render=True)\u003C\u002Fcode\u003E should be used instead.\nThe \u003Ccode\u003Estep()\u003C\u002Fcode\u003E method returns extra information about the player and the ball in the last tuple element. This extra information includes position (x, y) and velocity (x, y) for the ball and players and y rotation (in degrees) of the players.\u003C\u002Fp\u003E\n\u003Cp\u003EWe expose an RLLib-compatible multiagent interface.\nThis means, for example, that \u003Ccode\u003Eaction\u003C\u002Fcode\u003E should be a \u003Ccode\u003Edict\u003C\u002Fcode\u003E where keys are integers in \u003Ccode\u003E{0, 1, 2, 3}\u003C\u002Fcode\u003E corresponding to each agent.\nAdditionally, values should be single actions shaped like \u003Ccode\u003Eenv.action_space.shape\u003C\u002Fcode\u003E.\nObservations and rewards follow the same structure. Dones are only set for the key \u003Ccode\u003E__all__\u003C\u002Fcode\u003E, which means &quot;all agents&quot;.\nAgents 0 and 1 correspond to the blue team and agents 2 and 3 correspond to the orange team.\u003C\u002Fp\u003E\n\u003Cp\u003EHere&#39;s a full example:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E soccer_twos\n\nenv = soccer_twos.make(render=\u003Cspan class=\"hljs-literal\"\u003ETrue\u003C\u002Fspan\u003E)\n\u003Cspan class=\"hljs-built_in\"\u003Eprint\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-string\"\u003E&quot;Observation Space: &quot;\u003C\u002Fspan\u003E, env.observation_space.shape)\n\u003Cspan class=\"hljs-built_in\"\u003Eprint\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-string\"\u003E&quot;Action Space: &quot;\u003C\u002Fspan\u003E, env.action_space.shape)\n\nteam0_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\nteam1_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\n\u003Cspan class=\"hljs-keyword\"\u003Ewhile\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-literal\"\u003ETrue\u003C\u002Fspan\u003E:\n    obs, reward, done, info = env.step(\n        {\n            \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E: env.action_space.sample(),\n            \u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E: env.action_space.sample(),\n            \u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E: env.action_space.sample(),\n            \u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E: env.action_space.sample(),\n        }\n    )\n\n    team0_reward += reward[\u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E] + reward[\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E]\n    team1_reward += reward[\u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E] + reward[\u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E]\n    \u003Cspan class=\"hljs-keyword\"\u003Eif\u003C\u002Fspan\u003E done[\u003Cspan class=\"hljs-string\"\u003E&quot;__all__&quot;\u003C\u002Fspan\u003E]:\n        \u003Cspan class=\"hljs-built_in\"\u003Eprint\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-string\"\u003E&quot;Total Reward: &quot;\u003C\u002Fspan\u003E, team0_reward, \u003Cspan class=\"hljs-string\"\u003E&quot; x &quot;\u003C\u002Fspan\u003E, team1_reward)\n        team0_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\n        team1_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\n        env.reset()\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch4 id=\"environment-state-configuration\"\u003EEnvironment State Configuration\u003C\u002Fh4\u003E\n\u003Cp\u003EThe \u003Ccode\u003Eenv_channel\u003C\u002Fcode\u003E parameter allows for state configuration inside the simulation. To use it, you must first instantiate a \u003Ccode\u003Esoccer_twos.side_channels.EnvConfigurationChannel\u003C\u002Fcode\u003E and pass it in the \u003Ccode\u003Esoccer_twos.make\u003C\u002Fcode\u003E call. Here&#39;s a full example:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E soccer_twos\n\u003Cspan class=\"hljs-keyword\"\u003Efrom\u003C\u002Fspan\u003E soccer_twos.side_channels \u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E EnvConfigurationChannel\nenv_channel = EnvConfigurationChannel()\nenv = soccer_twos.make(env_channel=env_channel)\nenv.reset()\nenv_channel.set_parameters(\n    ball_state={\n        \u003Cspan class=\"hljs-string\"\u003E&quot;position&quot;\u003C\u002Fspan\u003E: [\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E, -\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E],\n        \u003Cspan class=\"hljs-string\"\u003E&quot;velocity&quot;\u003C\u002Fspan\u003E: [-\u003Cspan class=\"hljs-number\"\u003E1.2\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E],\n    },\n    players_states={\n        \u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E: {\n            \u003Cspan class=\"hljs-string\"\u003E&quot;position&quot;\u003C\u002Fspan\u003E: [-\u003Cspan class=\"hljs-number\"\u003E5\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E10\u003C\u002Fspan\u003E],\n            \u003Cspan class=\"hljs-string\"\u003E&quot;rotation_y&quot;\u003C\u002Fspan\u003E: \u003Cspan class=\"hljs-number\"\u003E45\u003C\u002Fspan\u003E,\n            \u003Cspan class=\"hljs-string\"\u003E&quot;velocity&quot;\u003C\u002Fspan\u003E: [\u003Cspan class=\"hljs-number\"\u003E5\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E],\n        }\n    }\n)\n\u003Cspan class=\"hljs-comment\"\u003E# env.step()\u003C\u002Fspan\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAll the \u003Ccode\u003Eenv_channel.set_parameters\u003C\u002Fcode\u003E method parameters and dict keys are optional. You can set a single parameter at a time or the full game state if you need so.\u003C\u002Fp\u003E\n\u003Ch3 id=\"evaluating\"\u003EEvaluating\u003C\u002Fh3\u003E\n\u003Cp\u003ETo quickly evaluate one agent against another and generate comprehensive statistics, you may use the \u003Ccode\u003Eevaluate\u003C\u002Fcode\u003E script:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Epython -m soccer_twos.evaluate -m1 agent_module -m2 opponent_module\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EYou can also provide the \u003Ccode\u003E--episodes\u003C\u002Fcode\u003E option to specify the number of episodes to evaluate on (defaults to 100).\u003C\u002Fp\u003E\n\u003Ch3 id=\"watching\"\u003EWatching\u003C\u002Fh3\u003E\n\u003Cp\u003ETo rollout via CLI, you must create an implementation (subclass) of \u003Ccode\u003Esoccer_twos.AgentInterface\u003C\u002Fcode\u003E and run:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Epython -m soccer_twos.watch -m agent_module\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EThis will run a human-friendly version of the environment, where your agent will play against itself.\nYou may instead use the options \u003Ccode\u003E-m1 agent_module -m2 opponent_module\u003C\u002Fcode\u003E to play against a different opponent.\nYou may also implement your own rollout script using \u003Ccode\u003Esoccer_twos.make(watch=True)\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"https:\u002F\u002Fraw.githubusercontent.com\u002Fbryanoliveira\u002Fsoccer-twos-env\u002Fmaster\u002Fimages\u002Fscreenshot.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch2 id=\"environment-specs\"\u003EEnvironment Specs\u003C\u002Fh2\u003E\n\u003Cp\u003EThis environment is based on Unity ML Agents&#39; \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FUnity-Technologies\u002Fml-agents\u002Fblob\u002F92ff2c26fef7174b443115454fa1c6045d622bc2\u002Fdocs\u002FLearning-Environment-Examples.md#soccer-twos\"\u003ESoccer Twos\u003C\u002Fa\u003E, so most of the specs are the same. Here, four agents compete in a 2 vs 2 toy soccer game, aiming to get the ball into the opponent&#39;s goal while preventing the ball from entering own goal.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"https:\u002F\u002Fraw.githubusercontent.com\u002Fbryanoliveira\u002Fsoccer-twos-env\u002Fmaster\u002Fimages\u002Fobs.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\u003Cbr\u002F\u003E\n\n\u003Cul\u003E\n\u003Cli\u003E  Observation space: a 336-dimensional vector corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees, each detecting 6 possible object types, along with the object&#39;s distance. The forward ray-casts contribute 264 state dimensions and backward 72 state dimensions over three observation stacks.\u003C\u002Fli\u003E\n\u003Cli\u003E  Action space: 3 discrete branched actions (MultiDiscrete) corresponding to forward, backward, sideways movement, as well as rotation (27 discrete actions).\u003C\u002Fli\u003E\n\u003Cli\u003EAgent Reward Function:\u003Cul\u003E\n\u003Cli\u003E  \u003Ccode\u003E1 - accumulated time penalty\u003C\u002Fcode\u003E: when ball enters opponent&#39;s goal. Accumulated time penalty is incremented by \u003Ccode\u003E(1 \u002F MaxSteps)\u003C\u002Fcode\u003E every fixed update and is reset to 0 at the beginning of an episode. In this build, \u003Ccode\u003EMaxSteps = 5000\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ccode\u003E-1\u003C\u002Fcode\u003E: when ball enters team&#39;s goal.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003ENote that while this is true when \u003Ccode\u003Evariation == EnvType.multiagent_player\u003C\u002Fcode\u003E, observation and action spaces may vary for other variations.\u003C\u002Fp\u003E\n",readingTime:i,title:"Multiagent Soccer Environment for Python",slug:"2021-09-05-soccer-twos-env",date:"2021-09-05",urls:[{cta:"Python Package",url:"https:\u002F\u002Fpypi.org\u002Fproject\u002Fsoccer-twos\u002F"},{cta:"Environment Code",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fsoccer-twos-env"},{cta:"Simulator Code",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-soccer"}],type:"Reinforcement Learning Environment",tags:[b,e,"rl"],image:"\u002Fimg\u002Fsoccer.gif",description:"A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA \u002F Deep Learning Brazil."},{html:"\u003Cp\u003EA \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCellular_automaton\"\u003ECellular Automata\u003C\u002Fa\u003E program built with C++, OpenGL, CUDA and OpenMP. It&#39;s built to run on a GPU but it also supports multithreaded CPU-only execution. On the right there&#39;s an example execution of \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FConway%27s_Game_of_Life\"\u003EConway&#39;s Game of Life\u003C\u002Fa\u003E on a 100x100 randomly initialised lattice.\u003C\u002Fp\u003E\n\u003Cp\u003EThe main objective of this project is to allow scaling up to a reasonably large number of cells while maintaining the code legibility and allowing for further customisations. It supports command-line arguments to set up quick configs (run \u003Ccode\u003E.\u002Fautomata -h\u003C\u002Fcode\u003E for details) like headless mode (which is significantly faster) and initial patterns (which can be loaded from the \u003Ccode\u003Epatterns\u003C\u002Fcode\u003E folder). It doesn&#39;t yet support the definition of evolution rules at runtime or lattice size inference, but I&#39;m working on that.\u003C\u002Fp\u003E\n\u003Cp\u003EThis program can currently evolve a dense &amp; high entropy 182.25 million cell Game of Life grid (13500x13500) with rendering enabled with up to 729 generations per second on a Ryzen 7 3700X \u002F RTX 3080 using up to 200MB RAM and 8.5GB VRAM (which is the actual scaling limiter).\u003C\u002Fp\u003E\n\u003Cp\u003EThe ability to evolve and render such large grids allows the program to run some really interesting patterns, like evolving the Game of Life \u003Cem\u003Ewithin\u003C\u002Fem\u003E the Game of Life:\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-75\" src=\"\u002Fimg\u002Fcellular_zoom.gif\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003EIn the GIF above we&#39;re running a 12300x12300 grid using Game of Life rules to evolve a pattern known as \u003Ca href=\"http:\u002F\u002Fb3s23life.blogspot.com\u002F2006_09_01_archive.html\"\u003EMeta-Toad\u003C\u002Fa\u003E. It uses a grid of \u003Ca href=\"https:\u002F\u002Fwww.conwaylife.com\u002Fwiki\u002FOTCA_metapixel\"\u003EOTCA Metapixels\u003C\u002Fa\u003E and requires about 35 thousand generations of the underlying automaton to represent a single generation of the meta-grid. The pattern being evolved by the meta-grid is known as \u003Ca href=\"https:\u002F\u002Fwww.conwaylife.com\u002Fwiki\u002FToad\"\u003EToad\u003C\u002Fa\u003E:\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fcellular-automata\u002Fraw\u002Fmaster\u002Fimages\u002Ftoad.gif\" width=\"150\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003EThis program also supports a benchmark mode (\u003Ccode\u003E-b\u003C\u002Fcode\u003E option), which outputs the total and average evolution and rendering timings to stdout. Combined with \u003Ccode\u003Ebenchmark.sh\u003C\u002Fcode\u003E and \u003Ccode\u003Ebenchmark_visualize.ipynb\u003C\u002Fcode\u003E, it is possible to plot speedups and evolution times for different lattice sizes. Currently, the GPU implementation achieves a speedup up to 627x over the single-core CPU implementation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cbr\u002F\u003E\n\u003Cimg class=\"text-img mw-50\" src=\"https:\u002F\u002Fraw.githubusercontent.com\u002Fbryanoliveira\u002Fcellular-automata\u002Fmaster\u002Fimages\u002Flat_hl_evo_speedup.png\"\u003E\n\u003Cimg class=\"text-img mw-50\" src=\"https:\u002F\u002Fraw.githubusercontent.com\u002Fbryanoliveira\u002Fcellular-automata\u002Fmaster\u002Fimages\u002Flat_hl_evo_avg.png\"\u003E\n\u003C\u002Fdiv\u003E\n\u003Cbr\u002F\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003ESpeedup over serial (left) and average grid evolution time in milliseconds (right) for lattice sizes 32x32, 64x64, ..., 4096x4096 and 1000 generations, using logarithmic X and Y axis. &quot;# Threads&quot; refers to the number of threads available for OpenMP CPU (Ryzen 7 3700X) runs while &quot;GPU&quot; refers to CUDA (RTX 3080) runs. For these tests, initial spawn probability was set to 0.5 and rendering was disabled.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Ch2 id=\"requirements\"\u003ERequirements\u003C\u002Fh2\u003E\n\u003Cp\u003ETo run the program you&#39;ll need:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Debian-like linux distro (I only tested this on Ubuntu 20)\u003C\u002Fli\u003E\n\u003Cli\u003EOpenGL* (GLEW, GLUT and GLM)\u003Cul\u003E\n\u003Cli\u003E  e.g. \u003Ccode\u003Esudo apt-get install libglew-dev freeglut3-dev libglm-dev\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fdeveloper.nvidia.com\u002Fcuda-downloads\"\u003ECUDA\u003C\u002Fa\u003E** (nvcc) and CUDA runtime libraries\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003ETo build it from source you&#39;ll also need:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003Eg++ (C++ 17) and \u003Cem\u003Emake\u003C\u002Fem\u003E\u003Cul\u003E\n\u003Cli\u003E  e.g. \u003Ccode\u003Esudo apt install build-essential\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E  Boost C++ Library (program_options module)\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fgabime\u002Fspdlog\"\u003Espdlog\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E*It is possible to run this program in headless-only mode, so if your machine doesn&#39;t support rendering (e.g. Colab runtimes) you may skip the OpenGL installation step. For that to work you must compile the program with the \u003Ccode\u003EHEADLESS_ONLY\u003C\u002Fcode\u003E flag set (e.g. \u003Ccode\u003Emake automata HEADLESS_ONLY=1\u003C\u002Fcode\u003E).\u003C\u002Fp\u003E\n\u003Cp\u003E**It is also possible to run this program in CPU-only mode, so if you don&#39;t have a CUDA-capable video card you may skip the CUDA installation step. For that to work you will need to compile the program with the \u003Ccode\u003ECPU_ONLY\u003C\u002Fcode\u003E flag set (e.g. \u003Ccode\u003Emake automata CPU_ONLY=1\u003C\u002Fcode\u003E).\u003C\u002Fp\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Ch3 id=\"building-from-source\"\u003EBuilding From Source\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Install the requirements\u003C\u002Fli\u003E\n\u003Cli\u003E  Clone this repository\u003C\u002Fli\u003E\n\u003Cli\u003EBuilding and executing:\u003Cul\u003E\n\u003Cli\u003E  Run \u003Ccode\u003Emake\u003C\u002Fcode\u003E to build and run\u003C\u002Fli\u003E\n\u003Cli\u003E  Run \u003Ccode\u003Emake automata\u003C\u002Fcode\u003E to build\u003C\u002Fli\u003E\n\u003Cli\u003E  Run \u003Ccode\u003Emake run\u003C\u002Fcode\u003E to run with default parameters\u003C\u002Fli\u003E\n\u003Cli\u003E  Run \u003Ccode\u003Emake clean\u003C\u002Fcode\u003E to remove generated files\u003C\u002Fli\u003E\n\u003Cli\u003E  Run \u003Ccode\u003Emake profile\u003C\u002Fcode\u003E to run \u003Ca href=\"https:\u002F\u002Fdeveloper.nvidia.com\u002Fnsight-systems\"\u003ENVIDIA&#39;s nsys\u003C\u002Fa\u003E profiling.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"executing-a-pre-built-binary-linux-x64--cuda-only\"\u003EExecuting a pre-built binary (Linux x64 + CUDA only)\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Download \u003Ccode\u003Ecellular-automata-linux64.zip\u003C\u002Fcode\u003E from the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fcellular-automata\u002Freleases\"\u003Elatest release\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  Extract the executable (\u003Ccode\u003Eautomata\u003C\u002Fcode\u003E) and the \u003Ccode\u003Epatterns\u003C\u002Fcode\u003E folder\u003C\u002Fli\u003E\n\u003Cli\u003E  Install OpenGL and CUDA from the requirements above\u003C\u002Fli\u003E\n\u003Cli\u003E  Run \u003Ccode\u003E.\u002Fautomata -h\u003C\u002Fcode\u003E to see all the available options\u003C\u002Fli\u003E\n\u003Cli\u003E  Run the program with \u003Ccode\u003E.\u002Fautomata --render\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EYou may want to set the number of available threads when running in CPU. For that, set the environment variable \u003Ccode\u003EOMP_NUM_THREADS\u003C\u002Fcode\u003E (e.g. \u003Ccode\u003Eenv OMP_NUM_THREADS=8 .\u002Fautomata -r\u003C\u002Fcode\u003E).\u003C\u002Fp\u003E\n\u003Cp\u003EIf your GPU has enough VRAM (&gt;= 8 GB), you may be able to reproduce the Meta-Toad simulation above. Run \u003Ccode\u003E.\u002Fautomata -r -x 12300 -y 12300 -p 0 -f patterns\u002Fmeta-toad.rle --skip-frames 80\u003C\u002Fcode\u003E to try it out!\u003C\u002Fp\u003E\n\u003Ch3 id=\"runtime-controls\"\u003ERuntime Controls\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003EBasic controls:\u003Cul\u003E\n\u003Cli\u003E  \u003Cstrong\u003Espace\u003C\u002Fstrong\u003E pauses\u002Fresumes the simulation;\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Cstrong\u003Eenter\u002Freturn\u003C\u002Fstrong\u003E runs a single generation;\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Cstrong\u003Eleft mouse click\u003C\u002Fstrong\u003E translates the grid relative to the max resolution\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Cstrong\u003Ectrl + left mouse click\u003C\u002Fstrong\u003E translates the camera relative to the world\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Cstrong\u003Emouse scroll\u003C\u002Fstrong\u003E zooms the grid in and out, relative to the max resolution\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Cstrong\u003Ectrl + mouse scroll\u003C\u002Fstrong\u003E zooms the camera, relative to the world\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Cstrong\u003Emiddle mouse click\u003C\u002Fstrong\u003E resets scale and translation\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"next-steps\"\u003ENext steps\u003C\u002Fh2\u003E\n\u003Cp\u003EThere is still much room for improvement. This includes better memory management, use of CPU parallelism and automated tests. My next steps include (but are not limited to):\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Addition of unit tests (in progress)\u003C\u002Fli\u003E\n\u003Cli\u003E  Usage of templates to abstract grid data types (e.g. cells should be represented with 1 bit instead of 8)\u003C\u002Fli\u003E\n\u003Cli\u003E  Usage of SM shared memory to explore data locality\u003C\u002Fli\u003E\n\u003Cli\u003E  Support for flexible rule definition\u003C\u002Fli\u003E\n\u003Cli\u003E  Support for infinite grids (e.g. storing only active cells)\u003C\u002Fli\u003E\n\u003Cli\u003E  Support for 3-D and N-D grids\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"references\"\u003EReferences\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E  What are \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCellular_automaton\"\u003ECellular Automata\u003C\u002Fa\u003E?\u003C\u002Fli\u003E\n\u003Cli\u003E  What is \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FConway%27s_Game_of_Life\"\u003EConway&#39;s Game of Life\u003C\u002Fa\u003E?\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"http:\u002F\u002Fgolly.sourceforge.net\u002F\"\u003EGolly\u003C\u002Fa\u003E: an open source cellular automata simulator that supports several Game of Life and other automata algorithms;\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fcopy.sh\u002Flife\u002F\"\u003ELife\u003C\u002Fa\u003E: an open source JavaScript implementation of Game of Life that runs in the browser;\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"http:\u002F\u002Fb3s23life.blogspot.com\u002F2006_09_01_archive.html\"\u003EConway&#39;s Life: Work in Progress\u003C\u002Fa\u003E: where I got the initial pattern for the Meta-Toad;\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fblog.amandaghassaei.com\u002F2020\u002F05\u002F01\u002Fthe-recursive-universe\u002F\"\u003EThe Recursive Universe\u003C\u002Fa\u003E: explores and explains how some of the meta-patterns work;\u003C\u002Fli\u003E\n\u003Cli\u003E  What are \u003Ca href=\"https:\u002F\u002Fwww.conwaylife.com\u002Fwiki\u002FOTCA_metapixel\"\u003EOTCA Metapixels\u003C\u002Fa\u003E?\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"bonus\"\u003EBonus\u003C\u002Fh2\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-100\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fcellular-automata\u002Fraw\u002Fmaster\u002Fimages\u002F1000x1000.gif\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003EA 1000x1000 randomly initialized grid running Game of life.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Chr\u003E\n\u003Cp\u003EThis program was developed during the 2021\u002F1 Parallel Computing (CCO0455) Computer Science graduate course at Universidade Federal de Goiás (UFG, Brazil).\u003C\u002Fp\u003E\n",readingTime:i,title:"Cellular Automata Framework",slug:"2021-03-10-cellular-automata",date:"2021-03-10",urls:[{cta:d,url:"\u002Fpdfs\u002FAutomatos_Celulares_CUDA_OpenMP.pdf"},{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fcellular-automata"},{cta:"Executable",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fcellular-automata\u002Freleases"}],type:"Project",tags:[b,"parallel","cuda","opengl",e],image:"\u002Fimg\u002Fcellular_zoom.gif",description:"A \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCellular_automaton\" target=\"_blank\"\u003ECellular Automata\u003C\u002Fa\u003E program built with C++, OpenGL, CUDA and OpenMP. The main objective of this project is to allow scaling up to a reasonably large number of cells while maintaining the code legibility and allowing for further customisations."},{html:"\u003Cp\u003EMachine Learning Algorithms have become increasingly efficient at solving complexreal-world problems. In particular, Reinforcement Learning algorithms are capable of learning behaviors applicable to robotics that can replace or work together with classical control models, thereby increasing their robustness, applicability and viability. However,it remains difficult to design reward functions that represent, for a reinforcement learning agent, the task it must perform. Recent research in this area proposes techniques such as curiosity and intrinsic motivation as an alternative to the use of extrinsic environmental rewards, proving to be efficient in guiding the agent to satisfactory exploration in game environments such as VizDoom and Super Mario Bros. This paper analyzes the impact of the intrinsic motivation technique on agent training in robotic simulation environments, as well as its general implications for aspects such as generalization, exploration and sampling efficiency. We found that this approach encourages increasing exploratory behaviors even after the goal tasks were learned. Furthermore, we found that adding information about other objects&#39; states into the agent&#39;s observation is crucial for learning complex behaviors when no dense reward signal is provided. This, however, requires the agent to learn it&#39;s own dynamics before interacting with the rest of the environment.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick.gif\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush.gif\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach.gif\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003ELearned policies for the tasks Pick And Place (left), Push (center) and Reach (right).\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003ETo read the full report, \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fblob\u002Fmaster\u002FMonografia.pdf\"\u003Eclick here\u003C\u002Fa\u003E (Portuguese).\u003C\u002Fp\u003E\n\u003Cp\u003EThis study was inspired by the \u003Ca href=\"https:\u002F\u002Fwww.aicrowd.com\u002Fchallenges\u002Frobot-open-ended-autonomous-learning-real\"\u003ERobot open-Ended Autonomous Learning\u003C\u002Fa\u003E competition. The theoretic background was mainly based on\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fpathak22.github.io\u002Fnoreward-rl\u002F\"\u003ECuriosity-driven Exploration by Self-supervised Prediction\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fpathak22.github.io\u002Flarge-scale-curiosity\u002F\"\u003ELarge-Scale Study of Curiosity-Driven Learning\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"success-rate-charts\"\u003ESuccess Rate Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch3 id=\"entropy-charts\"\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1811.11214\"\u003EEntropy\u003C\u002Fa\u003E Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick_ent.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush_ent.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach_ent.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch3 id=\"intrinsic-reward-charts\"\u003EIntrinsic Reward Charts\u003C\u002Fh3\u003E\n\u003Cp\u003EPick And Place Task (left), Push Task (center) and Reach (right).\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpick_int.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Fpush_int.png\"\u003E \n\u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fundergraduate-thesis\u002Fraw\u002Fmaster\u002Ffig\u002Fpreview\u002Freach_int.png\"\u003E\n\u003C\u002Fdiv\u003E\n",readingTime:c,title:"Intrinsic motivation for robotic manipulation learning with sparse rewards",slug:"2019-12-10-intrinsic-motivation",date:"2019-12-10",urls:[{cta:d,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fundergraduate-thesis\u002Fblob\u002Fmaster\u002FText%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf"}],type:"Undergraduate Thesis",tags:[f,h],image:"\u002Fimg\u002Fpick.gif",description:"Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."},{html:"\u003Cp\u003EThis is my code for the \u003Ca href=\"https:\u002F\u002Fwww.kaggle.com\u002Fc\u002Fbone-age-regression\"\u003EI2A2 Bone Age Regression competition\u003C\u002Fa\u003E. I learned a lot by building this pipeline from scratch and experimenting with different model architectures and optimizers. This was my first end-to-end image regression model, and it was very nice seeing my theoretical knowledge work in practice.\u003C\u002Fp\u003E\n\u003Cp\u003EThis competition was inspired by \u003Ca href=\"https:\u002F\u002Fwww.kaggle.com\u002Fkmader\u002Frsna-bone-age\"\u003ERSNA&#39;s Bone Age challenge\u003C\u002Fa\u003E, in which given hand X-ray images, the model should predict the patient&#39;s bone age.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fbone-age-regression\u002Fraw\u002Fmaster\u002Fdocs\u002Fex1.png\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fbone-age-regression\u002Fraw\u002Fmaster\u002Fdocs\u002Fex2.png\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fbone-age-regression\u002Fraw\u002Fmaster\u002Fdocs\u002Fex3.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003EX-ray images provided in the competition&#39;s dataset.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003EMy final solution used a \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1512.03385\"\u003EResNet50\u003C\u002Fa\u003E architecture, a \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1908.03265\"\u003ERectified Adam\u003C\u002Fa\u003E optimizer and geometric data augmentations. This model achieved a Mean Average Error of 13.2 after 20 epochs of training, which I believe could be improved given more training time and a better preprocessing pipeline (e.g. using object detection to segment the hands and normalizing hand rotation). Unfortunately, I didn&#39;t save all the hyperparameters I experimented with (neither their results), but you&#39;ll find the ones I used for my last submission in the code.\u003C\u002Fp\u003E\n\u003Cp\u003EI used \u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002Ftensorboard\"\u003Etensorboard\u003C\u002Fa\u003E to log the training curves and \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftqdm\u002Ftqdm\"\u003Etqdm\u003C\u002Fa\u003E to track progress. I also used \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Ffcm-notifier\"\u003EFCMNotifier\u003C\u002Fa\u003E, a tool I made to send logs as notifications to my phone.\u003C\u002Fp\u003E\n\u003Ch2 id=\"requirements\"\u003ERequirements\u003C\u002Fh2\u003E\n\u003Cp\u003ESee \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanlincoln\u002Fbone-age-regression\u002Fblob\u002Fmaster\u002Frequirements.txt\"\u003Erequirements.txt\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Download the requirements with \u003Ccode\u003Epip install -r requirements.txt\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  Download the dataset and sample submission with \u003Ccode\u003Esh download_data.sh\u003C\u002Fcode\u003E. You may need to log in with your Kaggle account in order to do it.\u003C\u002Fli\u003E\n\u003Cli\u003E  Train the ResNet50 model with \u003Ccode\u003Epython boneage.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  Try different models and hyperparameters by editing the training script or use the \u003Ccode\u003Eboneage.ipynb\u003C\u002Fcode\u003E notebook to do it interactively.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"credits\"\u003ECredits\u003C\u002Fh2\u003E\n\u003Cp\u003EI used the vision models already \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fpytorch\u002Fvision\u002Ftree\u002Fmaster\u002Ftorchvision\u002Fmodels\"\u003Eimplemented in torchvision\u003C\u002Fa\u003E with slight changes. You can try other torchvision models by adding the \u003Ccode\u003Ein_channels\u003C\u002Fcode\u003E parameter to generalize the number of input channels since torchvision models work with RGB images.\u003C\u002Fp\u003E\n",readingTime:c,title:"Bone Age Regression",slug:"2019-11-10-bone-age-regression",date:"2019-11-10",urls:[{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fbone-age-regression"}],type:"Deep Learning",tags:[b,"ai","deeplearning"],image:"\u002Fimg\u002Fbone.png",description:"This is my code for the \u003Ca href=\"https:\u002F\u002Fwww.kaggle.com\u002Fc\u002Fbone-age-regression\"\u003EI2A2 Bone Age Regression competition\u003C\u002Fa\u003E. I learned a lot by building this pipeline from scratch and experimenting with different model architectures and optimizers. This was my first end-to-end image regression model, and it was very nice seeing my theoretical knowledge work in practice."},{html:"\u003Cp\u003EQuack is a Unity3D game made for the Global Game Jam 2019 themed &quot;What home means to you?&quot; (\u003Ca href=\"https:\u002F\u002Fglobalgamejam.org\u002F2019\u002Fgames\u002Fquack\"\u003Echeck out the game&#39;s entry here\u003C\u002Fa\u003E).\nThe game consists of a happy chicken that wants to build a new home for its children. You have to collect sticks and group them on top of the main tree to make a lovely nest. This game was developed within 12 hours.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n \n\u003Cimg class=\"text-img mw-50\" src=\"https:\u002F\u002Fggj.s3.amazonaws.com\u002Fstyles\u002Fgame_sidebar__wide\u002Ffeatured_image\u002F2019\u002F01\u002F263393\u002Fmenu.png?itok=veRqhjix&timestamp=1548611925\"\u002F\u003E\n\u003Cimg class=\"text-img mw-50\" src=\"https:\u002F\u002Fggj.s3.amazonaws.com\u002Fstyles\u002Ffeature_image__wide\u002Fgames\u002Fscreenshots\u002Fingame3_9.png?itok=Xuf3MZan&timestamp=1548611547\"\u002F\u003E\n\n\u003C\u002Fdiv\u003E\n\n\u003Ch2 id=\"diversifiers\"\u003EDiversifiers\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Language-Independence - (Sponsored by Valve Software) - Create a game that can be understood regardless of which language the player speaks\u003C\u002Fli\u003E\n\u003Cli\u003E  Keep it simple - Make your game playable by people who can use no more than a D-pad plus 2 buttons, with hardware like an Xbox Adaptive Controller in mind.\u003C\u002Fli\u003E\n\u003Cli\u003E  Assetless - Create all visuals programmatically or in the scene editor, and avoid any importing of image files, sprite sheets, 3D models etc.\u003C\u002Fli\u003E\n\u003Cli\u003E  Super Secret Stash - Feature a hidden room within your game.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"credits\"\u003ECredits\u003C\u002Fh2\u003E\n\u003Cp\u003EThis game was made by a group of 3 computer science students located in Goiânia, Brazil:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Bryan (me, \u003Ca href=\"mailto:&#98;&#114;&#x79;&#97;&#110;&#x75;&#102;&#103;&#64;&#x67;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d;\"\u003E&#98;&#114;&#x79;&#97;&#110;&#x75;&#102;&#103;&#64;&#x67;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d;\u003C\u002Fa\u003E) programmed and modeled;\u003C\u002Fli\u003E\n\u003Cli\u003E  Luana (\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fluanagbmartins\"\u003E@luanagbmartins\u003C\u002Fa\u003E \u002F \u003Ca href=\"mailto:&#x6c;&#117;&#97;&#110;&#97;&#x67;&#x62;&#109;&#97;&#114;&#116;&#x69;&#x6e;&#x73;&#64;&#103;&#109;&#x61;&#x69;&#x6c;&#x2e;&#99;&#111;&#109;\"\u003E&#x6c;&#117;&#97;&#110;&#97;&#x67;&#x62;&#109;&#97;&#114;&#116;&#x69;&#x6e;&#x73;&#64;&#103;&#109;&#x61;&#x69;&#x6c;&#x2e;&#99;&#111;&#109;\u003C\u002Fa\u003E) programmed and designed the level;\u003C\u002Fli\u003E\n\u003Cli\u003E  Rennan (\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Frennan11\"\u003E@rennan11\u003C\u002Fa\u003E \u002F \u003Ca href=\"mailto:&#114;&#x65;&#110;&#110;&#97;&#110;&#x65;&#108;&#105;&#x74;&#64;&#103;&#x6d;&#97;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;\"\u003E&#114;&#x65;&#110;&#110;&#97;&#110;&#x65;&#108;&#105;&#x74;&#64;&#103;&#x6d;&#97;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;\u003C\u002Fa\u003E) designed the game and the sounds.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EIn-game sound effects downloaded from Soundsnap.\nIn-game font &quot;Gloria Hallelujah&quot; by Kimberly Geswein, downloaded from Google Fonts.\u003C\u002Fp\u003E\n",readingTime:c,title:"Quack",slug:"2019-01-10-quack",date:"2019-01-10",urls:[{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fgjams-2019-quack"},{cta:"Jam Entry",url:"https:\u002F\u002Fglobalgamejam.org\u002F2019\u002Fgames\u002Fquack"}],type:j,tags:[b,e,"jam",k],image:"\u002Fimg\u002Fquack.png",description:"Quack is a Unity3D game made for the Global Game Jam 2019 themed \"What home means to you?\". The game consists of a happy chicken that wants to build a new home for its children. You have to collect sticks and group them on top of the main tree to make a lovely nest. This game was developed within 12 hours."},{html:"\u003Cp\u003E3D Force simulator using only \u003Ca href=\"https:\u002F\u002Fprocessing.org\u002F\"\u003EProcessing\u003C\u002Fa\u003E&#39;s point() and line() functions. Uses Digital Differential Analyzer (DDA) to render lines between two points, Scan Line to render polygons, normal calculation to determine faces to render in 3D space and Newtonian physics. Written in Java.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"\u002Fimg\u002Frigidbody_physics.gif\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003EOne of the project&#39;s scenes, where the cube is affected by gravity and the ground is not.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003EThe program interface allows for real-time selection, positioning, rotation, scaling and acceleration of objects. In a \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fprocessing-3d-force-simulator\u002Fblob\u002Ffc899000baecf513cc3da4b38ab104cd4de260f7\u002FSimulator\u002FProjections.pde\"\u003Eprevious version\u003C\u002Fa\u003E it also supported selecting between Cavalier, Cabinet, Isometric, Perspective-Z and Perspective-XZ projections.\u003C\u002Fp\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Clone this repository\u003C\u002Fli\u003E\n\u003Cli\u003E  Install \u003Ca href=\"https:\u002F\u002Fprocessing.org\u002Fdownload\u002F\"\u003EProcessing\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  Open this project with Processing IDE or execute \u003Ccode\u003Eprocessing-java --sketch=Simulator --force --run\u003C\u002Fcode\u003E in a CLI.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Chr\u003E\n\u003Cp\u003EThis program was developed as the final project for the 2018\u002F2 Computer Graphics (INF0037) class of Computer Science at Universidade Federal de Goiás (UFG, Brazil).\u003C\u002Fp\u003E\n",readingTime:g,title:"3D Rendering & Force Simulator",slug:"2018-12-10-rigid-body-physics",date:"2018-12-10",urls:[{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fprocessing-3d-force-simulator"}],type:"Rendering",tags:[b,"rendering","simulation"],image:"\u002Fimg\u002Frigidbody_physics.gif",description:"3D Force simulator using only Processing's point() and line functions. Uses Digital Differential Analyzer (DDA) to render lines between two points, Scan Line to render polygons, normal calculation to determine faces to render in 3D space and Newtonian physics. Written in Java."},{html:"\u003Cp\u003EHi! This is the development repo of \u003Ca href=\"https:\u002F\u002Fwww.facebook.com\u002FNucleoPMec\u002F\"\u003EPequi Mecânico\u003C\u002Fa\u003E - INF&#39;s \u003Cstrong\u003EVery Small Size Soccer Team\u003C\u002Fstrong\u003E. Our team comprises several courses (Electrical Engineering, Computer Engineering, Software Engineering and Computer Science), all from Federal University of Goiás - \u003Ca href=\"https:\u002F\u002Fwww.ufg.br\u002F\"\u003EUFG\u003C\u002Fa\u003E - Goiânia. Our repository is open because we understand that our greatest job is to add our research and knowledge to the academic and industrial world.\u003C\u002Fp\u003E\n\u003Cp\u003EYou can find our Team Description Paper \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002FPY-VSSS-INF\u002Fblob\u002Fmaster\u002Fdocs\u002FTDP%20VSSS%20INF%202018.pdf\"\u003Ehere\u003C\u002Fa\u003E. We are open to answer any questions and suggestions through our email \u003Ca href=\"mailto:&#x70;&#101;&#113;&#117;&#105;&#109;&#x65;&#99;&#x61;&#x6e;&#x69;&#x63;&#111;&#x75;&#102;&#x67;&#64;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;\"\u003E&#x70;&#101;&#113;&#117;&#105;&#109;&#x65;&#99;&#x61;&#x6e;&#x69;&#x63;&#111;&#x75;&#102;&#x67;&#64;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"features\"\u003EFeatures\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Isolated modules for vision, strategy, control, communication, and interface\u003C\u002Fli\u003E\n\u003Cli\u003E  High-fidelity simulator made with MuJoCo\u003C\u002Fli\u003E\n\u003Cli\u003E  Qt interface\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=JQVrX5h7u_8\"\u003E\n        \u003Cimg class=\"text-img mw-100\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002FPY-VSSS-INF\u002Fraw\u002Fmaster\u002Fdocs\u002Fimages\u002FSimulator.gif\"\u002F\u003E\n    \u003C\u002Fa\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cblockquote\u003E\n\u003Cp\u003EOur simulator running two instances of the same control system. Green arrows indicate the robot&#39;s movement direction (given by the vector field), and blue, yellow, and red arrows indicate goalie, defender and attacker&#39;s targets.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Ch3 id=\"dependencies\"\u003EDependencies\u003C\u002Fh3\u003E\n\u003Cp\u003EOur code runs on all Python3-supported OS&#39;s (we recommend Python 3.4). To start using our software, you must install all requirements described on our requirements.txt file. You can easily do that by running:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Epip install -r requirements.txt\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EWe use MuJoCo as our simulation engine. Unfortunately, as MuJoCo is not free software, you must grab your license \u003Ca href=\"https:\u002F\u002Fwww.roboti.us\u002Flicense.html\"\u003Ehere\u003C\u002Fa\u003E to run our simulator.\u003C\u002Fp\u003E\n\u003Ch3 id=\"executing\"\u003EExecuting\u003C\u002Fh3\u003E\n\u003Cp\u003ETo open the GUI we use on our competitions, run:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Epython afrodite.py\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ETo open our simulator, run:\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ccode\u003Epython aether.py\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=UBV4qlAJ-sc\"\u003E\n        \u003Cimg class=\"text-img mw-100\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002FPY-VSSS-INF\u002Fraw\u002Fmaster\u002Fdocs\u002Fimages\u002FKick.gif\"\u002F\u003E\n    \u003C\u002Fa\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch2 id=\"social-networks\"\u003ESocial networks\u003C\u002Fh2\u003E\n\u003Cp\u003EOur activities and events always have updates. Follow us and get updated :D\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fwww.instagram.com\u002Fpequimecanico\u002F\"\u003EInstagram\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fwww.facebook.com\u002FNucleoPMec\"\u003EFacebook\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n",readingTime:c,title:"IEEE VSSS Team",slug:"2018-10-10-ieee-vsss-team",date:"2018-10-10",urls:[{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002FPY-VSSS-INF"},{cta:"Simulator Demo",url:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=JQVrX5h7u_8"},{cta:"Team Description Paper",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002FPY-VSSS-INF\u002Fblob\u002Fmaster\u002Fdocs\u002FTDP%20VSSS%20INF%202018.pdf"}],type:"Robotics",tags:[b,"robotics","leadership"],image:"\u002Fimg\u002Fvsss_cover.gif",description:"A stack consisting of image processing, computer vision, team coordination, navigation, control and communication software to compete in the 2018's Latin-American Robotics Competition for the Pequi Mecânico UFG - INF's team."},{html:"\u003Cp\u003EIn a world overrun by zombies, you must survive as long as possible and defeat different bosses while unlocking weapons, upgrades, equipment, characters, maps and much more. Reviving the classics of the &#39;80s and &#39;90s, Die Zombit is a retro wave top-down shooting game with an amazing soundtrack and addictive gameplay that guarantees many hours of fun. Check out its trailer \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=sO7FSZ3TJns\"\u003Ehere\u003C\u002Fa\u003E:\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"img\u002Fzombit_walter.gif\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003EI made this game during my first year in the Computer Science course, using C# and UnityScript. I&#39;ve learned a lot from this project, from software engineering to game design and a little bit of pixel art. However, I believe the project could be significantly improved using better design patterns, clearer abstractions and algorithmic complexity in mind. Yet, it is a fun game that you can \u003Ca href=\"https:\u002F\u002Fplay.google.com\u002Fstore\u002Fapps\u002Fdetails?id=com.elitgames.zombit\"\u003Edownload\u003C\u002Fa\u003E (an older version) in Play Store.\u003C\u002Fp\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-zombit\u002Fraw\u002Fmaster\u002FImages\u002F3.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-zombit\u002Fraw\u002Fmaster\u002FImages\u002F2.png\"\u003E \n    \u003Cimg class=\"text-img mw-33\" src=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-zombit\u002Fraw\u002Fmaster\u002FImages\u002F1.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Ch3 id=\"running-a-pre-built-binary-android-and-windows-only\"\u003ERunning a pre-built binary (Android and Windows only)\u003C\u002Fh3\u003E\n\u003Cp\u003EFor the Android version, you may \u003Ca href=\"https:\u002F\u002Fplay.google.com\u002Fstore\u002Fapps\u002Fdetails?id=com.elitgames.zombit\"\u003Ehead to the Play Store\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EFor the Windows version, do the following:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EDownload Die.Zombit.[version].zip from the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-zombit\u002Freleases\"\u003Elatest release\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003EExtract the game folder\u003C\u002Fli\u003E\n\u003Cli\u003EDouble click to run it.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"buildingrunning-from-source\"\u003EBuilding\u002Frunning from source\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Install Unity3D (tested for v2019.3)\u003C\u002Fli\u003E\n\u003Cli\u003E  Open the project folder\u003C\u002Fli\u003E\n\u003Cli\u003E  Hit Play and have fun!\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"next-steps\"\u003ENext steps\u003C\u002Fh2\u003E\n\u003Cp\u003EThis version is an unfinished project that rethinks the UI, weapon management and overall objectives. It was made as an indie game event demo and some things are broken. The fixes and new features I plan to add include:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E  Fix joystick and touchscreen support\u003C\u002Fli\u003E\n\u003Cli\u003E  Fix pause menu UI\u003C\u002Fli\u003E\n\u003Cli\u003E  Fix end game UI\u003C\u002Fli\u003E\n\u003Cli\u003E  Add multiplayer support\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"more-gifs\"\u003EMore GIFs\u003C\u002Fh2\u003E\n\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"img\u002Fzombit_monster.gif\"\u002F\u003E\u003Cbr\u002F\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"img\u002Fzombit_round_up.gif\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch2 id=\"credits\"\u003ECredits\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003ESoundtracks by\u003Cul\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fsoundcloud.com\u002Fbossfightswe\"\u003EBossfight\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fsoundcloud.com\u002Fultrasyd\"\u003EUltrasyd\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fsoundcloud.com\u002Fdunderpatrullen\"\u003EDunderpatrullen\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E  \u003Ca href=\"https:\u002F\u002Fsoundcloud.com\u002Fdetiouss\"\u003EDetious\u003C\u002Fa\u003E &amp; \u003Ca href=\"https:\u002F\u002Fsoundcloud.com\u002Flockyn\"\u003ELockyn\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n",readingTime:c,title:"Die Zombit",slug:"2015-06-10-die-zombit",date:"2015-06-10",urls:[{cta:a,url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-zombit"},{cta:"Store",url:"https:\u002F\u002Fplay.google.com\u002Fstore\u002Fapps\u002Fdetails?id=com.elitgames.zombit"},{cta:"Trailer",url:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=sO7FSZ3TJns"}],type:j,tags:[b,e,k],image:"\u002Fimg\u002Fzombit_cover.gif",description:"Reviving the classics of the 80's and 90's, Die Zombit is a retrowave top-down shooting game that has a striking soundtrack and an addictive gameplay which guarantee many hours of fun."}]}}("Code","project","2 min read","Paper","game","research","1 min read","publication","6 min read","Game","indie"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.62f806c2.js"}catch(e){main="/client/legacy/client.0a19eb85.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/PostListItem-a56df87b.css rel=stylesheet><link href=client/index-ca4a0f66.css rel=stylesheet> <title>Blog</title> <link href=/client/client.62f806c2.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/index.171537d7.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.d1630470.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/PostListItem.10dd6943.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/PostListItem-a56df87b.css rel=preload as=style><link href=/client/index-ca4a0f66.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a href=/ class=back rel=prefetch>« home</a> <div class="mb-5 mt-5 pb-3 text-center"><h1 class=mt-2>Blog</h1></div> <div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2023-02-debt-pricing-slr class=svelte-1povu5z rel=prefetch>Data-Driven Debt Pricing: A Systematic Literature Review</a></h3> <small class="svelte-1povu5z text-muted">Research Paper · February 2023</small> <p class="indicate_blank item-description">This review explores the potential of machine learning in debt pricing, with a focus on reinforcement learning. It concludes that more research is needed and highlights issues with reproducibility and comparability of results. <a href=blog/2023-02-debt-pricing-slr class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2023-02-debt-pricing-slr class=svelte-1povu5z rel=prefetch><img alt="Data-Driven Debt Pricing: A Systematic Literature Review" class=svelte-1povu5z src=/img/deal.png></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2021-10-25-pulse-rl class=svelte-1povu5z rel=prefetch>PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning</a></h3> <small class="svelte-1povu5z text-muted">Workshop Publication · October 2021</small> <p class="indicate_blank item-description">Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning - Presentation at the 2nd Offline Reinforcement Learning Workshop at the 35th Conference on Neural Information Processing (NeurIPS 2021). <a href=blog/2021-10-25-pulse-rl class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2021-10-25-pulse-rl class=svelte-1povu5z rel=prefetch><img alt="PulseRL: Enabling Offline Reinforcement Learning for Digital Marketing Systems via Conservative Q-Learning" class=svelte-1povu5z src=/img/pulserl.png></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2021-09-05-soccer-twos-env class=svelte-1povu5z rel=prefetch>Multiagent Soccer Environment for Python</a></h3> <small class="svelte-1povu5z text-muted">Reinforcement Learning Environment · September 2021</small> <p class="indicate_blank item-description">A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil. <a href=blog/2021-09-05-soccer-twos-env class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2021-09-05-soccer-twos-env class=svelte-1povu5z rel=prefetch><img alt="Multiagent Soccer Environment for Python" class=svelte-1povu5z src=/img/soccer.gif></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2021-03-10-cellular-automata class=svelte-1povu5z rel=prefetch>Cellular Automata Framework</a></h3> <small class="svelte-1povu5z text-muted">Project · March 2021</small> <p class="indicate_blank item-description">A <a href=https://en.wikipedia.org/wiki/Cellular_automaton target=_blank>Cellular Automata</a> program built with C++, OpenGL, CUDA and OpenMP. The main objective of this project is to allow scaling up to a reasonably large number of cells while maintaining the code legibility and allowing for further customisations. <a href=blog/2021-03-10-cellular-automata class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2021-03-10-cellular-automata class=svelte-1povu5z rel=prefetch><img alt="Cellular Automata Framework" class=svelte-1povu5z src=/img/cellular_zoom.gif></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2019-12-10-intrinsic-motivation class=svelte-1povu5z rel=prefetch>Intrinsic motivation for robotic manipulation learning with sparse rewards</a></h3> <small class="svelte-1povu5z text-muted">Undergraduate Thesis · December 2019</small> <p class="indicate_blank item-description">Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments. <a href=blog/2019-12-10-intrinsic-motivation class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2019-12-10-intrinsic-motivation class=svelte-1povu5z rel=prefetch><img alt="Intrinsic motivation for robotic manipulation learning with sparse rewards" class=svelte-1povu5z src=/img/pick.gif></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2019-11-10-bone-age-regression class=svelte-1povu5z rel=prefetch>Bone Age Regression</a></h3> <small class="svelte-1povu5z text-muted">Deep Learning · November 2019</small> <p class="indicate_blank item-description">This is my code for the <a href=https://www.kaggle.com/c/bone-age-regression>I2A2 Bone Age Regression competition</a>. I learned a lot by building this pipeline from scratch and experimenting with different model architectures and optimizers. This was my first end-to-end image regression model, and it was very nice seeing my theoretical knowledge work in practice. <a href=blog/2019-11-10-bone-age-regression class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2019-11-10-bone-age-regression class=svelte-1povu5z rel=prefetch><img alt="Bone Age Regression" class=svelte-1povu5z src=/img/bone.png></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2019-01-10-quack class=svelte-1povu5z rel=prefetch>Quack</a></h3> <small class="svelte-1povu5z text-muted">Game · January 2019</small> <p class="indicate_blank item-description">Quack is a Unity3D game made for the Global Game Jam 2019 themed "What home means to you?". The game consists of a happy chicken that wants to build a new home for its children. You have to collect sticks and group them on top of the main tree to make a lovely nest. This game was developed within 12 hours. <a href=blog/2019-01-10-quack class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2019-01-10-quack class=svelte-1povu5z rel=prefetch><img alt=Quack class=svelte-1povu5z src=/img/quack.png></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2018-12-10-rigid-body-physics class=svelte-1povu5z rel=prefetch>3D Rendering & Force Simulator</a></h3> <small class="svelte-1povu5z text-muted">Rendering · December 2018</small> <p class="indicate_blank item-description">3D Force simulator using only Processing's point() and line functions. Uses Digital Differential Analyzer (DDA) to render lines between two points, Scan Line to render polygons, normal calculation to determine faces to render in 3D space and Newtonian physics. Written in Java. <a href=blog/2018-12-10-rigid-body-physics class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2018-12-10-rigid-body-physics class=svelte-1povu5z rel=prefetch><img alt="3D Rendering & Force Simulator" class=svelte-1povu5z src=/img/rigidbody_physics.gif></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2018-10-10-ieee-vsss-team class=svelte-1povu5z rel=prefetch>IEEE VSSS Team</a></h3> <small class="svelte-1povu5z text-muted">Robotics · October 2018</small> <p class="indicate_blank item-description">A stack consisting of image processing, computer vision, team coordination, navigation, control and communication software to compete in the 2018's Latin-American Robotics Competition for the Pequi Mecânico UFG - INF's team. <a href=blog/2018-10-10-ieee-vsss-team class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2018-10-10-ieee-vsss-team class=svelte-1povu5z rel=prefetch><img alt="IEEE VSSS Team" class=svelte-1povu5z src=/img/vsss_cover.gif></a></div> </div><div class="svelte-1povu5z mb-5 post-item row"><div class=col-md-8><h3 class="svelte-1povu5z text-left"> <a href=blog/2015-06-10-die-zombit class=svelte-1povu5z rel=prefetch>Die Zombit</a></h3> <small class="svelte-1povu5z text-muted">Game · June 2015</small> <p class="indicate_blank item-description">Reviving the classics of the 80's and 90's, Die Zombit is a retrowave top-down shooting game that has a striking soundtrack and an addictive gameplay which guarantee many hours of fun. <a href=blog/2015-06-10-die-zombit class=svelte-1povu5z rel=prefetch target=_blank>Read more </a></div> <div class="text-center col-md-4"><a href=blog/2015-06-10-die-zombit class=svelte-1povu5z rel=prefetch><img alt="Die Zombit" class=svelte-1povu5z src=/img/zombit_cover.gif></a></div> </div></div> <hr> <footer class="text-center indicate_blank"><small class="text-muted horizontal-name-photo svelte-1sdunwv"><a href=. class=no-underline><img alt="Bryan Oliveira" class=svelte-1sdunwv src=/img/me.jpg id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small></footer></main> </div></div> 