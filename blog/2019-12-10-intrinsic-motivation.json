{"html":"<p>As my undergraduate thesis, I studied the impact of curiosity and intrinsic motivation as exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments. We found that this approach encourages increasing exploratory behaviours even after the goal tasks were learned. Furthermore, we found that adding information about other objects&#39; states into the agent&#39;s observation is crucial for learning complex behaviours when no dense reward signal is provided. This study was inspired by the <a href=\"https://www.aicrowd.com/challenges/robot-open-ended-autonomous-learning-real\">Robot open-Ended Autonomous Learning</a> competition.</p>\n<p>To read the full report, <a href=\"https://github.com/bryanlincoln/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf\">click here</a> (Portuguese).</p>\n<p><img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.gif\" width=\"230\" height=\"150\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.gif\" width=\"230\" height=\"150\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.gif\" width=\"230\" height=\"150\"></p>\n<blockquote>\n<p>Learned policies for the tasks Pick And Place (left), Push (center) and Reach (right).</p>\n</blockquote>\n<h2 id=\"requirements\">Requirements</h2>\n<ul>\n<li>  <a href=\"https://docs.python.org/\">Python 3</a></li>\n<li>  <a href=\"http://pytorch.org/\">PyTorch</a></li>\n<li>  <a href=\"https://github.com/openai/gym\">OpenAI Gym</a></li>\n<li>  <a href=\"https://github.com/openai/baselines\">OpenAI baselines</a></li>\n<li>  <a href=\"https://github.com/jmichaux/gym-fetch\">Gym Fetch</a></li>\n</ul>\n<h2 id=\"usage\">Usage</h2>\n<p>To run the code, simply execute <code>python main.py</code> after installing all the requirements. There are many customizable hyperparemeters and configurations. You can see them with <code>python main.py --help</code>. The exact hyperparameters for this study&#39;s experiments can be found in the folder <code>experiments</code>.</p>\n<h2 id=\"credits\">Credits</h2>\n<p>This code was based on and adapted from</p>\n<ul>\n<li>  <a href=\"https://github.com/jmichaux/intrinsic-motivation\">Jon Michaux&#39;s implementation of intrinsic motivation</a> (which was used as starting point for running the experiments of this repo)</li>\n<li>  <a href=\"https://github.com/srama2512/curiosity-driven-exploration\">Santhosh Ramakrishnan&#39;s implementation of curiosity</a></li>\n<li>  <a href=\"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\">Ilya Kostrikov&#39; implementation of recent RL algorithms</a></li>\n<li>  <a href=\"https://github.com/openai/baselines\">OpenAI Baselines</a></li>\n<li>  <a href=\"https://github.com/cbschaff/pytorch-dl\">Chip Schaff&#39;s Deep Learning Library</a></li>\n</ul>\n<p>The inspiration and theoretic background was mainly based on</p>\n<ul>\n<li>  <a href=\"https://pathak22.github.io/noreward-rl/\">Curiosity-driven Exploration by Self-supervised Prediction</a></li>\n<li>  <a href=\"https://pathak22.github.io/large-scale-curiosity/\">Large-Scale Study of Curiosity-Driven Learning</a></li>\n</ul>\n<h2 id=\"results\">Results</h2>\n<h3 id=\"success-rate-charts\">Success Rate Charts</h3>\n<p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p>\n<p><img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick.png\" width=\"230\" height=\"170\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push.png\" width=\"230\" height=\"170\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach.png\" width=\"230\" height=\"170\"></p>\n<h3 id=\"entropy-charts\"><a href=\"https://arxiv.org/abs/1811.11214\">Entropy</a> Charts</h3>\n<p>Pick And Place Task (left), Push Task (center) and Reach (right). Blue lines are results for vanilla PPO (baseline) and red lines for PPO + intrinsic motivation.</p>\n<p><img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_ent.png\" width=\"230\" height=\"170\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_ent.png\" width=\"230\" height=\"170\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_ent.png\" width=\"230\" height=\"170\"></p>\n<h3 id=\"intrinsic-reward-charts\">Intrinsic Reward Charts</h3>\n<p>Pick And Place Task (left), Push Task (center) and Reach (right).</p>\n<p><img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/pick_int.png\" width=\"230\" height=\"170\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/push_int.png\" width=\"230\" height=\"170\"> <img src=\"https://github.com/bryanlincoln/undergraduate-thesis/raw/master/fig/preview/reach_int.png\" width=\"230\" height=\"170\"></p>\n<p>The interpretation of these curves can be found in my full report.</p>\n","readingTime":"2 min read","title":"Intrinsic motivation for robotic manipulation learning with sparse rewards","slug":"2019-12-10-intrinsic-motivation","date":"2019-12-10","urls":[{"cta":"Code","url":"https://github.com/bryanoliveira/intrinsic-motivation"},{"cta":"Paper","url":"https://github.com/bryanoliveira/undergraduate-thesis/blob/master/Text%20-%20Intrinsic%20motivation%20for%20robotic%20manipulation%20learning%20with%20sparse%20rewards.pdf"}],"type":"Undergraduate Thesis","tags":["research","publication"],"image":"/img/pick.gif","description":"Intrinsic motivation for robotic manipulation learning with sparse rewards - Study of the impact of curiosity and intrinsic motivation as an exploration strategy for deep reinforcement learning agents on sparse-reward robotic manipulator environments."}