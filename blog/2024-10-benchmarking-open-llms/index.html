<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"> <meta name=author content="Bryan Oliveira"> <meta name=theme-color content=#FFFFFF> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{html:"\u003Ch2 id=\"abstract\"\u003EAbstract\u003C\u002Fh2\u003E\n\u003Cp\u003EUsing Large Language Models (LLMs) to evaluate other LLMs is becoming a scalable alternative to human assessment. While promising, this approach faces challenges such as positional bias and fairness. We provide a comprehensive evaluation of open-source LLMs using benchmarks for instruction adherence and positional bias. Our results show that open models are rapidly closing the gap with proprietary models like GPT-4. Although some open-source models match GPT-4 in metrics like extraction success rates, GPT-4 still leads in overall consistency and fairness. This study highlights the potential of open-source models in evaluation tasks while identifying areas where they still lag behind proprietary counterparts.\u003C\u002Fp\u003E\n",readingTime:"1 min read",title:"Benchmarking Open-Source LLMs as Model Evaluators",slug:a,date:"2024-10-16",urls:[],type:"Research Paper",tags:["research"],image:"\u002Fimg\u002Fjudge_eval.webp",description:"We evaluate open-source LLMs against proprietary models using benchmarks for instruction adherence and positional bias, finding that open models are closing the performance gap with GPT-4, though GPT-4 still leads in overall consistency and fairness."},slug:a}}("2024-10-benchmarking-open-llms"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.59272086.js"}catch(e){main="/client/legacy/client.bd4762a4.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/[slug]-a70e5dcf.css rel=stylesheet> <title>Benchmarking Open-Source LLMs as Model Evaluators</title><link href=https://bryanoliveira.github.io/blog/2024-10-benchmarking-open-llms rel=canonical data-svelte=svelte-17660c7><meta data-svelte=svelte-17660c7 name=Description content="We evaluate open-source LLMs against proprietary models using benchmarks for instruction adherence and positional bias, finding that open models are closing the performance gap with GPT-4, though GPT-4 still leads in overall consistency and fairness."><meta data-svelte=svelte-17660c7 content=article property=og:type><meta data-svelte=svelte-17660c7 content="Benchmarking Open-Source LLMs as Model Evaluators" property=og:title><meta data-svelte=svelte-17660c7 content=https://bryanoliveira.github.io/blog/2024-10-benchmarking-open-llms property=og:url><meta data-svelte=svelte-17660c7 content="We evaluate open-source LLMs against proprietary models using benchmarks for instruction adherence and positional bias, finding that open models are closing the performance gap with GPT-4, though GPT-4 still leads in overall consistency and fairness." property=og:description><meta data-svelte=svelte-17660c7 content=/img/judge_eval.webp property=og:image><meta data-svelte=svelte-17660c7 name=image content=/img/judge_eval.webp><meta data-svelte=svelte-17660c7 name=twitter:card content=summary_large_image><meta data-svelte=svelte-17660c7 name=twitter:domain value=bryanoliveira.github.io><meta data-svelte=svelte-17660c7 name=twitter:creator value=bryanoliveira_><meta data-svelte=svelte-17660c7 name=twitter:title value="Benchmarking Open-Source LLMs as Model Evaluators"><meta data-svelte=svelte-17660c7 name=twitter:description content="We evaluate open-source LLMs against proprietary models using benchmarks for instruction adherence and positional bias, finding that open models are closing the performance gap with GPT-4, though GPT-4 still leads in overall consistency and fairness."><meta data-svelte=svelte-17660c7 name=twitter:image content=/img/judge_eval.webp><meta data-svelte=svelte-17660c7 name=twitter:label1 value="Published on"><meta data-svelte=svelte-17660c7 name=twitter:data1 value="Oct 16, 2024"><meta data-svelte=svelte-17660c7 name=twitter:label2 value="Reading Time"><meta data-svelte=svelte-17660c7 name=twitter:data2 value="1 min read"> <link href=/client/client.59272086.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/[slug].a2187622.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.11293038.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/[slug]-a70e5dcf.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a class=back href=/blog rel=prefetch>« posts</a> <div class="text-center mb-5 mt-5"><h1>Benchmarking Open-Source LLMs as Model Evaluators</h1> <div class="text-muted mt-4">Research Paper · <span title=10/16/2024>October 2024</span> · 1 min read</div></div> <div class=row><div class=col-md-8><p>We evaluate open-source LLMs against proprietary models using benchmarks for instruction adherence and positional bias, finding that open models are closing the performance gap with GPT-4, though GPT-4 still leads in overall consistency and fairness.</p> <div class="text-center mb-3 text-md-left"></div></div> <div class=col-md-4><img alt="Benchmarking Open-Source LLMs as Model Evaluators" class="svelte-qbrnkn cover-img" src=/img/judge_eval.webp></div></div> <hr> <div class="svelte-qbrnkn post"><h2 id=abstract>Abstract</h2> <p>Using Large Language Models (LLMs) to evaluate other LLMs is becoming a scalable alternative to human assessment. While promising, this approach faces challenges such as positional bias and fairness. We provide a comprehensive evaluation of open-source LLMs using benchmarks for instruction adherence and positional bias. Our results show that open models are rapidly closing the gap with proprietary models like GPT-4. Although some open-source models match GPT-4 in metrics like extraction success rates, GPT-4 still leads in overall consistency and fairness. This study highlights the potential of open-source models in evaluation tasks while identifying areas where they still lag behind proprietary counterparts.</p> </div> <hr> <footer class="text-center indicate_blank"><small class="svelte-1sdunwv horizontal-name-photo text-muted"><a class=no-underline href=.><img alt="Bryan Oliveira" class=svelte-1sdunwv src=/img/me.jpg id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small> </footer></main> </div></div> 