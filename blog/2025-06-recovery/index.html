<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"> <meta name=author content="Bryan Oliveira"> <meta name=theme-color content=#FFFFFF> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{html:"\u003Ch2 id=\"abstract\"\u003EAbstract\u003C\u002Fh2\u003E\n\u003Cp\u003ETraditional static discount policies in debt recovery often fail to adapt to diverse debtor behaviors and evolving market dynamics. This research developed, evaluated, and deployed a comprehensive reinforcement learning (RL) system for optimizing discount policies to maximize recovered debt and minimize negotiation costs within a large financial institution. Our methodology encompassed developing sophisticated lifetime value (LTV) models as dynamic reward functions, implementing multi-armed bandit (MAB) meta-policies for autonomous policy evaluation and selection, and exploring diverse RL approaches including Imitation Learning and Offline RL. Key findings demonstrate superior performance of RL-driven discount policies, achieving lower average discounts and higher collection values in production compared to established baselines. The LTV models proved crucial for handling delayed feedback, while MAB meta-policies effectively orchestrated policy deployment in live operational settings. This work demonstrates the practical viability of applying advanced RL techniques to real-world financial challenges.\u003C\u002Fp\u003E\n\u003Chr\u003E\n\u003Cp\u003EPresented at the Workshop on Practical Insights into Reinforcement Learning for Real Systems at the 2nd Reinforcement Learning Conference (RLC).\u003C\u002Fp\u003E\n",readingTime:"1 min read",title:"Reinforcement Learning for Debt Pricing: A Case Study in Financial Services",slug:a,date:"2025-06-13",urls:[{cta:"Paper",url:"https:\u002F\u002Fopenreview.net\u002Fforum?id=1cfG46owm8"}],type:"Workshop on RL4RS @ RLC 2025",tags:["research","featured"],image:"\u002Fimg\u002Frecovery_traffic_percentage.jpg",description:"Offline RL with LTV-based rewards and bandit orchestration at a large financial institution improved collection values."},slug:a}}("2025-06-recovery"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.d93174ea.js"}catch(e){main="/client/legacy/client.ceb7b0a2.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/[slug]-a70e5dcf.css rel=stylesheet> <title>Reinforcement Learning for Debt Pricing: A Case Study in Financial Services</title><link href=https://bryanoliveira.github.io/blog/2025-06-recovery rel=canonical data-svelte=svelte-17660c7><meta data-svelte=svelte-17660c7 name=Description content="Offline RL with LTV-based rewards and bandit orchestration at a large financial institution improved collection values."><meta data-svelte=svelte-17660c7 content=article property=og:type><meta data-svelte=svelte-17660c7 content="Reinforcement Learning for Debt Pricing: A Case Study in Financial Services" property=og:title><meta data-svelte=svelte-17660c7 content=https://bryanoliveira.github.io/blog/2025-06-recovery property=og:url><meta data-svelte=svelte-17660c7 content="Offline RL with LTV-based rewards and bandit orchestration at a large financial institution improved collection values." property=og:description><meta data-svelte=svelte-17660c7 content=/img/recovery_traffic_percentage.jpg property=og:image><meta data-svelte=svelte-17660c7 name=image content=/img/recovery_traffic_percentage.jpg><meta data-svelte=svelte-17660c7 name=twitter:card content=summary_large_image><meta data-svelte=svelte-17660c7 name=twitter:domain value=bryanoliveira.github.io><meta data-svelte=svelte-17660c7 name=twitter:creator value=bryanoliveira_><meta data-svelte=svelte-17660c7 name=twitter:title value="Reinforcement Learning for Debt Pricing: A Case Study in Financial Services"><meta data-svelte=svelte-17660c7 name=twitter:description content="Offline RL with LTV-based rewards and bandit orchestration at a large financial institution improved collection values."><meta data-svelte=svelte-17660c7 name=twitter:image content=/img/recovery_traffic_percentage.jpg><meta data-svelte=svelte-17660c7 name=twitter:label1 value="Published on"><meta data-svelte=svelte-17660c7 name=twitter:data1 value="Jun 13, 2025"><meta data-svelte=svelte-17660c7 name=twitter:label2 value="Reading Time"><meta data-svelte=svelte-17660c7 name=twitter:data2 value="1 min read"> <link href=/client/client.d93174ea.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/[slug].ea8b2f91.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.1a9df1d7.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/[slug]-a70e5dcf.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a class=back href=/blog rel=prefetch>« posts</a> <div class="text-center mb-5 mt-5"><h1>Reinforcement Learning for Debt Pricing: A Case Study in Financial Services</h1> <div class="text-muted mt-4">Workshop on RL4RS @ RLC 2025 · <span title=6/13/2025>June 2025</span> · 1 min read</div></div> <div class=row><div class=col-md-8><p>Offline RL with LTV-based rewards and bandit orchestration at a large financial institution improved collection values.</p> <div class="text-center mb-3 text-md-left"><a class="no-underline btn btn-secondary btn-sm mb-1 mr-1" href="https://openreview.net/forum?id=1cfG46owm8" target=_blank>Paper ⧉ </a></div></div> <div class=col-md-4><img alt="Reinforcement Learning for Debt Pricing: A Case Study in Financial Services" class="svelte-qbrnkn cover-img" src=/img/recovery_traffic_percentage.jpg></div></div> <hr> <div class="svelte-qbrnkn post"><h2 id=abstract>Abstract</h2> <p>Traditional static discount policies in debt recovery often fail to adapt to diverse debtor behaviors and evolving market dynamics. This research developed, evaluated, and deployed a comprehensive reinforcement learning (RL) system for optimizing discount policies to maximize recovered debt and minimize negotiation costs within a large financial institution. Our methodology encompassed developing sophisticated lifetime value (LTV) models as dynamic reward functions, implementing multi-armed bandit (MAB) meta-policies for autonomous policy evaluation and selection, and exploring diverse RL approaches including Imitation Learning and Offline RL. Key findings demonstrate superior performance of RL-driven discount policies, achieving lower average discounts and higher collection values in production compared to established baselines. The LTV models proved crucial for handling delayed feedback, while MAB meta-policies effectively orchestrated policy deployment in live operational settings. This work demonstrates the practical viability of applying advanced RL techniques to real-world financial challenges.</p> <hr> <p>Presented at the Workshop on Practical Insights into Reinforcement Learning for Real Systems at the 2nd Reinforcement Learning Conference (RLC).</p> </div> <hr> <footer class="text-center indicate_blank"><small class="svelte-1sdunwv horizontal-name-photo text-muted"><a class=no-underline href=.><img alt="Bryan Oliveira" class=svelte-1sdunwv src=/img/me.jpg id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small> </footer></main> </div></div> 