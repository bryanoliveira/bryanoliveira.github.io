<!DOCTYPE html> <html lang=en> <head> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169339523-1"></script> <script> window.dataLayer = window.dataLayer || []
            function gtag() {
                dataLayer.push(arguments)
            }
            gtag('js', new Date())

            gtag('config', 'UA-169339523-1') </script> <meta charset=utf-8> <meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"> <meta name=author content="Bryan Oliveira"> <meta name=theme-color content=#FFFFFF> <title>Bryan Oliveira</title> <base href=/ > <link href=/manifest.json rel=manifest crossorigin=use-credentials> <link href=/img/icon.png rel=icon type=image/png> <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400&display=swap" rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css rel=stylesheet> <link href=/css/fonts.css rel=stylesheet> <link href=/css/bootstrap.css rel=stylesheet> <link href=/css/global.css rel=stylesheet> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{html:"\u003Cdiv align=\"center\"\u003E\n    \u003Cimg class=\"text-img mw-100\" src=\"https:\u002F\u002Fraw.githubusercontent.com\u002Fbryanoliveira\u002Fsoccer-twos-env\u002Fmaster\u002Fimages\u002Fscreenshot.png\"\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp\u003EPre-compiled versions of this environment are available for Linux, Windows and MacOS (x86, 64 bits). The source code for this environment is available \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Funity-soccer\"\u003Ehere\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"requirements\"\u003ERequirements\u003C\u002Fh2\u003E\n\u003Cp\u003ESee \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fsoccer-twos-env\u002Fblob\u002Fmaster\u002Frequirements.txt\"\u003Erequirements.txt\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"usage\"\u003EUsage\u003C\u002Fh2\u003E\n\u003Ch3 id=\"for-training\"\u003EFor training\u003C\u002Fh3\u003E\n\u003Cp\u003EImport this package and instantiate the environment:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E soccer_twos\n\nenv = soccer_twos.make()\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThe \u003Ccode\u003Emake\u003C\u002Fcode\u003E method accepts several options:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EOption\u003C\u002Fth\u003E\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Erender\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EWhether to render the environment. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ewatch\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EWhether to run an audience-friendly version the provided Soccer-Twos environment. Forces \u003Ccode\u003Erender\u003C\u002Fcode\u003E to \u003Ccode\u003ETrue\u003C\u002Fcode\u003E, \u003Ccode\u003Etime_scale\u003C\u002Fcode\u003E to \u003Ccode\u003E1\u003C\u002Fcode\u003E and \u003Ccode\u003Equality_level\u003C\u002Fcode\u003E to \u003Ccode\u003E5\u003C\u002Fcode\u003E. Has no effect when \u003Ccode\u003Eenv_path\u003C\u002Fcode\u003E is set. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Evariation\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EA soccer env variation in EnvType. Defaults to \u003Ccode\u003EEnvType.multiagent_player\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Etime_scale\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe time scale to use for the environment. This should be less than \u003Ccode\u003E100\u003C\u002Fcode\u003Ex for better simulation accuracy. Defaults to \u003Ccode\u003E20\u003C\u002Fcode\u003Ex realtime.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Equality_level\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe quality level to use when rendering the environment. Ranges between \u003Ccode\u003E0\u003C\u002Fcode\u003E (lowest) and \u003Ccode\u003E5\u003C\u002Fcode\u003E (highest). Defaults to \u003Ccode\u003E0\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ebase_port\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe base port to use to communicate with the environment. Defaults to \u003Ccode\u003E50039\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eworker_id\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EUsed as base port shift to avoid communication conflicts. Defaults to \u003Ccode\u003E0\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eenv_path\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe path to the environment executable. Overrides \u003Ccode\u003Ewatch\u003C\u002Fcode\u003E. Defaults to the provided Soccer-Twos environment.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eflatten_branched\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EIf \u003Ccode\u003ETrue\u003C\u002Fcode\u003E, turn branched discrete action spaces into a \u003Ccode\u003EDiscrete\u003C\u002Fcode\u003E space rather than \u003Ccode\u003EMultiDiscrete\u003C\u002Fcode\u003E. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eopponent_policy\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EThe policy to use for the opponent when \u003Ccode\u003Evariation==team_vs_policy\u003C\u002Fcode\u003E. Defaults to a random agent.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Esingle_player\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003EWhether to let the agent control a single player, while the other stays still. Only works when \u003Ccode\u003Evariation==team_vs_policy\u003C\u002Fcode\u003E. Defaults to \u003Ccode\u003EFalse\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003EThe created \u003Ccode\u003Eenv\u003C\u002Fcode\u003E exposes a basic \u003Ca href=\"https:\u002F\u002Fgym.openai.com\u002F\"\u003EGym\u003C\u002Fa\u003E interface.\nNamely, the methods \u003Ccode\u003Ereset()\u003C\u002Fcode\u003E, \u003Ccode\u003Estep(action: Dict[int, np.ndarray])\u003C\u002Fcode\u003E and \u003Ccode\u003Eclose()\u003C\u002Fcode\u003E are available.\nThe \u003Ccode\u003Erender()\u003C\u002Fcode\u003E method has currently no effect and \u003Ccode\u003Esoccer_twos.make(render=True)\u003C\u002Fcode\u003E should be used instead.\nThe \u003Ccode\u003Estep()\u003C\u002Fcode\u003E method returns extra information about the player and the ball in the last tuple element.\nThis information may be used to build custom reward functions if needed.\u003C\u002Fp\u003E\n\u003Cp\u003EWe expose an RLLib-compatible multiagent interface.\nThis means, for example, that \u003Ccode\u003Eaction\u003C\u002Fcode\u003E should be a \u003Ccode\u003Edict\u003C\u002Fcode\u003E where keys are integers in \u003Ccode\u003E{0, 1, 2, 3}\u003C\u002Fcode\u003E corresponding to each agent.\nAdditionally, values should be single actions shaped like \u003Ccode\u003Eenv.action_space.shape\u003C\u002Fcode\u003E.\nObservations and rewards follow the same structure. Dones are only set for the key \u003Ccode\u003E__all__\u003C\u002Fcode\u003E, which means &quot;all agents&quot;.\nAgents 0 and 1 correspond to the blue team and agents 2 and 3 correspond to the orange team.\u003C\u002Fp\u003E\n\u003Cp\u003EHere&#39;s a full example:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Eimport\u003C\u002Fspan\u003E soccer_twos\n\nenv = soccer_twos.make(render=\u003Cspan class=\"hljs-literal\"\u003ETrue\u003C\u002Fspan\u003E)\n\u003Cspan class=\"hljs-built_in\"\u003Eprint\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-string\"\u003E&quot;Observation Space: &quot;\u003C\u002Fspan\u003E, env.observation_space.shape)\n\u003Cspan class=\"hljs-built_in\"\u003Eprint\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-string\"\u003E&quot;Action Space: &quot;\u003C\u002Fspan\u003E, env.action_space.shape)\n\nteam0_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\nteam1_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\n\u003Cspan class=\"hljs-keyword\"\u003Ewhile\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-literal\"\u003ETrue\u003C\u002Fspan\u003E:\n    obs, reward, done, info = env.step(\n        {\n            \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E: env.action_space.sample(),\n            \u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E: env.action_space.sample(),\n            \u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E: env.action_space.sample(),\n            \u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E: env.action_space.sample(),\n        }\n    )\n\n    team0_reward += reward[\u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E] + reward[\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E]\n    team1_reward += reward[\u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E] + reward[\u003Cspan class=\"hljs-number\"\u003E3\u003C\u002Fspan\u003E]\n    \u003Cspan class=\"hljs-keyword\"\u003Eif\u003C\u002Fspan\u003E done[\u003Cspan class=\"hljs-string\"\u003E&quot;__all__&quot;\u003C\u002Fspan\u003E]:\n        \u003Cspan class=\"hljs-built_in\"\u003Eprint\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-string\"\u003E&quot;Total Reward: &quot;\u003C\u002Fspan\u003E, team0_reward, \u003Cspan class=\"hljs-string\"\u003E&quot; x &quot;\u003C\u002Fspan\u003E, team1_reward)\n        team0_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\n        team1_reward = \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E\n        env.reset()\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EMore information about the environment including reward functions and observation spaces can be found \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FUnity-Technologies\u002Fml-agents\u002Fblob\u002F92ff2c26fef7174b443115454fa1c6045d622bc2\u002Fdocs\u002FLearning-Environment-Examples.md#soccer-twos\"\u003Ehere\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch3 id=\"watching--evaluating\"\u003EWatching \u002F evaluating\u003C\u002Fh3\u003E\n\u003Cp\u003EYou may implement your own rollout script using \u003Ccode\u003Esoccer_twos.make(watch=True)\u003C\u002Fcode\u003E or use our CLI tool.\nTo rollout via CLI, you must create an implementation (subclass) of \u003Ccode\u003Esoccer_twos.AgentInterface\u003C\u002Fcode\u003E and run \u003Ccode\u003Epython -m soccer_twos.watch -m agent_module\u003C\u002Fcode\u003E.\nThis will run a human-friendly version of the environment, where your agent will play against itself.\nYou may instead run \u003Ccode\u003Epython -m soccer_twos.watch -m1 agent_module -m2 opponent_module\u003C\u002Fcode\u003E to play against a different opponent.\u003C\u002Fp\u003E\n",readingTime:"3 min read",title:"Multiagent Soccer Environment for Python",slug:a,date:"2021-09-05",urls:[{cta:"Python Package",url:"https:\u002F\u002Fpypi.org\u002Fproject\u002Fceia-soccer-twos\u002F"},{cta:"Code",url:"https:\u002F\u002Fgithub.com\u002Fbryanoliveira\u002Fsoccer-twos-env"}],type:"Reinforcement Learning Environment",tags:["project","game","rl"],image:"\u002Fimg\u002Fsoccer.gif",description:"A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA \u002F Deep Learning Brazil."},slug:a}}("2021-09-05-soccer-twos-env"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.1a55b524.js"}catch(e){main="/client/legacy/client.d5c0fbcc.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@2.0.4.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> <link href=client/client-f256ac86.css rel=stylesheet><link href=client/HorizontalNamePhoto-83c2ef81.css rel=stylesheet><link href=client/[slug]-a70e5dcf.css rel=stylesheet> <title>Multiagent Soccer Environment for Python</title><link href=https://bryanoliveira.github.io/blog/2021-09-05-soccer-twos-env rel=canonical data-svelte=svelte-1oc1ljd><meta data-svelte=svelte-1oc1ljd content=article property=og:type><meta data-svelte=svelte-1oc1ljd content="Multiagent Soccer Environment for Python" property=og:title><meta data-svelte=svelte-1oc1ljd name=Description content="A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil."><meta data-svelte=svelte-1oc1ljd content="A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil." property=og:description><meta data-svelte=svelte-1oc1ljd name=image content=/img/soccer.gif property=og:image><meta data-svelte=svelte-1oc1ljd name=twitter:card content=summary_large_image><meta data-svelte=svelte-1oc1ljd name=twitter:domain value=bryanoliveira.github.io><meta data-svelte=svelte-1oc1ljd name=twitter:creator value=bryanoliveira_><meta data-svelte=svelte-1oc1ljd name=twitter:title value="Multiagent Soccer Environment for Python"><meta data-svelte=svelte-1oc1ljd name=twitter:description content="A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil."><meta data-svelte=svelte-1oc1ljd name=twitter:image content=/img/soccer.gif><meta data-svelte=svelte-1oc1ljd name=twitter:label1 value="Published on"><meta data-svelte=svelte-1oc1ljd name=twitter:data1 value="Sep 4, 2021"><meta data-svelte=svelte-1oc1ljd name=twitter:label2 value="Reading Time"><meta data-svelte=svelte-1oc1ljd name=twitter:data2 value="3 min read"> <link href=/client/client.1a55b524.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/client-f256ac86.css rel=preload as=style><link href=/client/[slug].752b3bd7.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto.62a48600.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/inject_styles.5607aec6.js rel=modulepreload as=script crossorigin=use-credentials><link href=/client/HorizontalNamePhoto-83c2ef81.css rel=preload as=style><link href=/client/[slug]-a70e5dcf.css rel=preload as=style></head> <body> <div id=sapper> <div class="svelte-e1wq04 cover-container d-flex flex-column mt-5 mx-auto p-3 w-100"><main class="mb-5 cover svelte-e1wq04"> <a href=/blog class=back rel=prefetch>« posts</a> <div class="text-center mb-5 mt-5"><h1>Multiagent Soccer Environment for Python</h1> <div class="text-muted mt-4">Reinforcement Learning Environment · <span title=9/4/2021>September 2021</span> · 3 min read</div></div> <div class=row><div class=col-md-8><p>A pre-compiled Soccer-Twos environment with multi-agent Gym-compatible wrappers and a human-friendly visualizer. Built on top of Unity ML Agents to be used as final assignment for the Reinforcement Learning Minicourse at CEIA / Deep Learning Brazil.</p> <div class="text-center mb-3 text-md-left"><a href=https://pypi.org/project/ceia-soccer-twos/ class="no-underline btn btn-secondary btn-sm mr-1" target=_blank>Python Package ⧉ </a><a href=https://github.com/bryanoliveira/soccer-twos-env class="no-underline btn btn-secondary btn-sm mr-1" target=_blank>Code ⧉ </a></div></div> <div class=col-md-4><img class="svelte-qbrnkn cover-img" src=/img/soccer.gif alt="Multiagent Soccer Environment for Python"></div></div> <hr> <div class="svelte-qbrnkn post"><div align=center> <img class="mw-100 text-img" src=https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png> </div> <p>Pre-compiled versions of this environment are available for Linux, Windows and MacOS (x86, 64 bits). The source code for this environment is available <a href=https://github.com/bryanoliveira/unity-soccer>here</a>.</p> <h2 id=requirements>Requirements</h2> <p>See <a href=https://github.com/bryanoliveira/soccer-twos-env/blob/master/requirements.txt>requirements.txt</a>.</p> <h2 id=usage>Usage</h2> <h3 id=for-training>For training</h3> <p>Import this package and instantiate the environment:</p> <pre><code class=language-python><span class=hljs-keyword>import</span> soccer_twos

env = soccer_twos.make()
</code></pre> <p>The <code>make</code> method accepts several options:</p> <table> <thead> <tr> <th>Option</th> <th>Description</th> </tr> </thead> <tr> <td><code>render</code></td> <td>Whether to render the environment. Defaults to <code>False</code>.</td> </tr> <tr> <td><code>watch</code></td> <td>Whether to run an audience-friendly version the provided Soccer-Twos environment. Forces <code>render</code> to <code>True</code>, <code>time_scale</code> to <code>1</code> and <code>quality_level</code> to <code>5</code>. Has no effect when <code>env_path</code> is set. Defaults to <code>False</code>.</td> </tr> <tr> <td><code>variation</code></td> <td>A soccer env variation in EnvType. Defaults to <code>EnvType.multiagent_player</code></td> </tr> <tr> <td><code>time_scale</code></td> <td>The time scale to use for the environment. This should be less than <code>100</code>x for better simulation accuracy. Defaults to <code>20</code>x realtime.</td> </tr> <tr> <td><code>quality_level</code></td> <td>The quality level to use when rendering the environment. Ranges between <code>0</code> (lowest) and <code>5</code> (highest). Defaults to <code>0</code>.</td> </tr> <tr> <td><code>base_port</code></td> <td>The base port to use to communicate with the environment. Defaults to <code>50039</code>.</td> </tr> <tr> <td><code>worker_id</code></td> <td>Used as base port shift to avoid communication conflicts. Defaults to <code>0</code>.</td> </tr> <tr> <td><code>env_path</code></td> <td>The path to the environment executable. Overrides <code>watch</code>. Defaults to the provided Soccer-Twos environment.</td> </tr> <tr> <td><code>flatten_branched</code></td> <td>If <code>True</code>, turn branched discrete action spaces into a <code>Discrete</code> space rather than <code>MultiDiscrete</code>. Defaults to <code>False</code>.</td> </tr> <tr> <td><code>opponent_policy</code></td> <td>The policy to use for the opponent when <code>variation==team_vs_policy</code>. Defaults to a random agent.</td> </tr> <tr> <td><code>single_player</code></td> <td>Whether to let the agent control a single player, while the other stays still. Only works when <code>variation==team_vs_policy</code>. Defaults to <code>False</code>.</td> </tr> </table> <p>The created <code>env</code> exposes a basic <a href=https://gym.openai.com/ >Gym</a> interface. Namely, the methods <code>reset()</code>, <code>step(action: Dict[int, np.ndarray])</code> and <code>close()</code> are available. The <code>render()</code> method has currently no effect and <code>soccer_twos.make(render=True)</code> should be used instead. The <code>step()</code> method returns extra information about the player and the ball in the last tuple element. This information may be used to build custom reward functions if needed.</p> <p>We expose an RLLib-compatible multiagent interface. This means, for example, that <code>action</code> should be a <code>dict</code> where keys are integers in <code>{0, 1, 2, 3}</code> corresponding to each agent. Additionally, values should be single actions shaped like <code>env.action_space.shape</code>. Observations and rewards follow the same structure. Dones are only set for the key <code>__all__</code>, which means "all agents". Agents 0 and 1 correspond to the blue team and agents 2 and 3 correspond to the orange team.</p> <p>Here's a full example:</p> <pre><code class=language-python><span class=hljs-keyword>import</span> soccer_twos

env = soccer_twos.make(render=<span class=hljs-literal>True</span>)
<span class=hljs-built_in>print</span>(<span class=hljs-string>"Observation Space: "</span>, env.observation_space.shape)
<span class=hljs-built_in>print</span>(<span class=hljs-string>"Action Space: "</span>, env.action_space.shape)

team0_reward = <span class=hljs-number>0</span>
team1_reward = <span class=hljs-number>0</span>
<span class=hljs-keyword>while</span> <span class=hljs-literal>True</span>:
    obs, reward, done, info = env.step(
        {
            <span class=hljs-number>0</span>: env.action_space.sample(),
            <span class=hljs-number>1</span>: env.action_space.sample(),
            <span class=hljs-number>2</span>: env.action_space.sample(),
            <span class=hljs-number>3</span>: env.action_space.sample(),
        }
    )

    team0_reward += reward[<span class=hljs-number>0</span>] + reward[<span class=hljs-number>1</span>]
    team1_reward += reward[<span class=hljs-number>2</span>] + reward[<span class=hljs-number>3</span>]
    <span class=hljs-keyword>if</span> done[<span class=hljs-string>"__all__"</span>]:
        <span class=hljs-built_in>print</span>(<span class=hljs-string>"Total Reward: "</span>, team0_reward, <span class=hljs-string>" x "</span>, team1_reward)
        team0_reward = <span class=hljs-number>0</span>
        team1_reward = <span class=hljs-number>0</span>
        env.reset()
</code></pre> <p>More information about the environment including reward functions and observation spaces can be found <a href=https://github.com/Unity-Technologies/ml-agents/blob/92ff2c26fef7174b443115454fa1c6045d622bc2/docs/Learning-Environment-Examples.md#soccer-twos>here</a>.</p> <h3 id=watching--evaluating>Watching / evaluating</h3> <p>You may implement your own rollout script using <code>soccer_twos.make(watch=True)</code> or use our CLI tool. To rollout via CLI, you must create an implementation (subclass) of <code>soccer_twos.AgentInterface</code> and run <code>python -m soccer_twos.watch -m agent_module</code>. This will run a human-friendly version of the environment, where your agent will play against itself. You may instead run <code>python -m soccer_twos.watch -m1 agent_module -m2 opponent_module</code> to play against a different opponent.</p> </div> <hr> <footer class="text-center indicate_blank"><small class="svelte-1sdunwv horizontal-name-photo text-muted"><a href=. class=no-underline><img class=svelte-1sdunwv src=/img/me.jpg alt="Bryan Oliveira" id=img-me> <h2 class=svelte-1sdunwv>Bryan Oliveira</h2></a> </small> </footer></main> </div></div> 